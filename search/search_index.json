{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#grupo-404-name_not_found","title":"Grupo 404 - Name_Not_Found","text":""},{"location":"#integrantes","title":"Integrantes","text":"<ol> <li>Guilherme  Orlandi de Oliveira</li> <li>Luis Felipe Galina Degaspari</li> <li>Luiz Felipe Pimenta Berrettini</li> <li>Luiz Fernando Pazdziora Costa</li> </ol> <p>Estudantes do 4\u00b0 semestre de Ci\u00eancias de Dados e Neg\u00f3cios (CDN) na ESPM (Escola Superior de Propaganda e Marketing). </p> <p>Projetos de machine learning realizados em 2025.2, orientados e supervisionados pelo professor Humberto Sandmann.</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Projeto 1 - Data 30/09/2025</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"projeto/main/","title":"Projeto 1","text":""},{"location":"projeto/main/#modelo-de-machine-learning-arvore-de-decisoes-knn-e-k-means","title":"Modelo de Machine Learning - \u00c1rvore de Decis\u00f5es, KNN e K-Means","text":"<p>Para esse projeto, foi utilizado um dataset obtido no Kaggle. Os dados usados podem ser baixados aqui, e foram adaptados de outra base de vinhos, </p>"},{"location":"projeto/main/#objetivo","title":"Objetivo","text":"<p>O dataset possui informa\u00e7\u00f5es diversas sobre resultados de an\u00e1lises qu\u00edmicas de vinhos produzidos na mesma regi\u00e3o da It\u00e1lia, mas derivados de tr\u00eas diferentes cultivares. A an\u00e1lise determinou as quantidade de 13 constituintes encontrados em cada um dos tr\u00eas tipos de vinho. O objetivo da an\u00e1lise \u00e9 clusterizar a base atrav\u00e9s do k-means e, com os modelos supervisionados, prever o tipo de vinho com base nos dados fornecidos.</p>"},{"location":"projeto/main/#workflow","title":"Workflow","text":"<p>Os pontos \"etapas\" s\u00e3o o passo-a-passo da realiza\u00e7\u00e3o do projeto.</p>"},{"location":"projeto/main/#etapa-1-exploracao-de-dados","title":"Etapa 1 - Explora\u00e7\u00e3o de Dados","text":"<p>Primeiramente, para entender melhor a base de dados, vamos descobrir quantas linhas e colunas o dataset possui.</p> Sa\u00eddaC\u00f3digo <p>(178, 13)</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nprint(df.shape)\n</code></pre> <p>Como foi poss\u00edvel observar no c\u00f3digo acima, o dataset possui 178 linhas e 13 colunas, com cada linha possuindo os dados de um vinho.</p>"},{"location":"projeto/main/#colunas-do-dataset","title":"Colunas do Dataset","text":"<p>Em seguida, \u00e9 necess\u00e1rio descobrir a natureza dos dados. Isso ser\u00e1 feito rodando as linhas de c\u00f3digo abaixo:</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nprint(df.info())\n</code></pre> <p>As informa\u00e7\u00f5es obtidas foram as seguintes:</p> Coluna Tipo Descri\u00e7\u00e3o <code>Alcohol</code> Float Teor alco\u00f3lico do vinho <code>Malic_Acid</code> Float \u00c1cido m\u00e1lico <code>Ash</code> Float Cinzas <code>Ash_Alcanity</code> Float Alcalinidade das cinzas <code>Magnesium</code> Inteiro Magn\u00e9sio <code>Total_Phenols</code> Float Total de fen\u00f3is <code>Flavanoids</code> Float Flavan\u00f3ides <code>Nonflavanoid_Phenols</code> Float Fen\u00f3is n\u00e3o-flavan\u00f3ides <code>Proanthocyanins</code> Float Proantocianidinas <code>Color_Intensity</code> Float Intensidade da cor do vinho <code>Hue</code> Float Satura\u00e7\u00e3o do vinho <code>OD280</code> Float Rela\u00e7\u00e3o OD280/OD315 de vinhos dilu\u00eddos <code>Proline</code> Inteiro Prolina"},{"location":"projeto/main/#visualizacao-das-variaveis","title":"Visualiza\u00e7\u00e3o das vari\u00e1veis","text":"<p>Em seguida, \u00e9 essencial realizar gr\u00e1ficos para visualizar como cada uma das vari\u00e1veis se comportam, com o objetivo de entender melhor a base da dados. Todas vari\u00e1veis da base s\u00e3o quantitativas, sendo onze cont\u00ednuas e duas discretas.</p>"},{"location":"projeto/main/#variaveis-quantitativas-continuas","title":"Vari\u00e1veis Quantitativas Cont\u00ednuas","text":"<p>Para cada uma das vari\u00e1veis num\u00e9ricas cont\u00ednuas, ser\u00e1 feito um histograma com o objetivo de visualizar a frequ\u00eancia de valores.</p> AlcoholMalic_AcidAshAsh_AlcanityTotal_PhenolsFlavanoidsNonflavanoid_PhenolsProanthocyaninsColor_IntensityHueOD280 Gr\u00e1ficoC\u00f3digo 2025-10-03T12:14:10.038057 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Alcohol\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"red\")\nplt.title(\"Distribui\u00e7\u00e3o do Teor Alco\u00f3lico dos Vinhos - Histograma\")\nplt.xlabel(\"Teor Alco\u00f3lico\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-03T12:14:10.233080 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Malic_Acid\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"yellow\")\nplt.title(\"Distribui\u00e7\u00e3o de \u00c1cido M\u00e1lico dos Vinhos - Histograma\")\nplt.xlabel(\"\u00c1cido M\u00e1lico\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-03T12:14:10.475099 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Ash\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"green\")\nplt.title(\"Distribui\u00e7\u00e3o de Cinzas dos Vinhos - Histograma\")\nplt.xlabel(\"Cinzas\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-03T12:14:10.677054 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Ash_Alcanity\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"blue\")\nplt.title(\"Distribui\u00e7\u00e3o da Alcalinidade das Cinzas dos Vinhos - Histograma\")\nplt.xlabel(\"Alcalinidade das Cinzas\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-03T12:14:10.897614 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Total_Phenols\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"purple\")\nplt.title(\"Distribui\u00e7\u00e3o do Total de Fen\u00f3is dos Vinhos - Histograma\")\nplt.xlabel(\"Total de Fen\u00f3is\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-03T12:14:11.109683 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Flavanoids\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"orange\")\nplt.title(\"Distribui\u00e7\u00e3o de Flavan\u00f3ides dos Vinhos - Histograma\")\nplt.xlabel(\"Flavan\u00f3ides\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-03T12:14:11.308665 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Nonflavanoid_Phenols\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"lightblue\")\nplt.title(\"Distribui\u00e7\u00e3o de Fen\u00f3is n\u00e3o Flavan\u00f3ides dos Vinhos - Histograma\")\nplt.xlabel(\"Fen\u00f3is n\u00e3o Flavan\u00f3ides\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-03T12:14:11.498649 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Proanthocyanins\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"brown\")\nplt.title(\"Distribui\u00e7\u00e3o de Proantocianidinas dos Vinhos - Histograma\")\nplt.xlabel(\"Proantocianidinas\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-03T12:14:11.700332 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Color_Intensity\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"grey\")\nplt.title(\"Distribui\u00e7\u00e3o de Intensidade da Cor dos Vinhos - Histograma\")\nplt.xlabel(\"Intensidade da Cor\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-03T12:14:11.893097 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Hue\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"lightgreen\")\nplt.title(\"Distribui\u00e7\u00e3o de Satura\u00e7\u00e3o dos Vinhos - Histograma\")\nplt.xlabel(\"Satura\u00e7\u00e3o\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-03T12:14:12.100364 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"OD280\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"pink\")\nplt.title(\"Distribui\u00e7\u00e3o do OD280 dos Vinhos - Histograma\")\nplt.xlabel(\"OD280\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre>"},{"location":"projeto/main/#variaveis-quantitativas-discretas","title":"Vari\u00e1veis Quantitativas Discretas","text":"<p>Para ambas vari\u00e1veis num\u00e9ricas discretas, tamb\u00e9m faremos histogramas.</p> MagnesiumProline Gr\u00e1ficoC\u00f3digo 2025-10-03T12:14:12.278032 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Magnesium\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"lightgreen\")\nplt.title(\"Distribui\u00e7\u00e3o de Magn\u00e9sio dos Vinhos - Histograma\")\nplt.xlabel(\"Magn\u00e9sio\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-10-03T12:14:12.501000 image/svg+xml Matplotlib v3.10.6, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Proline\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"skyblue\")\nplt.title(\"Distribui\u00e7\u00e3o de Prolina dos Vinhos - Histograma\")\nplt.xlabel(\"Prolina\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> <p>Atrav\u00e9s das an\u00e1lises, foi poss\u00edvel alcan\u00e7ar uma compreens\u00e3o mais aprofundada do funcionamento de cada uma das vari\u00e1veis no dataset, al\u00e9m de haver insights valiosos nesses gr\u00e1ficos.</p>"},{"location":"projeto/main/#etapa-2-pre-processamento","title":"Etapa 2 - Pr\u00e9-processamento","text":""},{"location":"projeto/main/#1-passo-identificacao-de-valores-nulos","title":"1\u00b0 Passo: Identifica\u00e7\u00e3o de valores nulos","text":"<p>Atrav\u00e9s da linha de c\u00f3digo abaixo, pode-se identificar que n\u00e3o h\u00e1 valores nulos na base. Portanto, pularemos o passo de tratamento de valores nulos.</p> <pre><code>print(df.isnull().sum())\n</code></pre>"},{"location":"projeto/main/#2-passo-remocao-de-colunas-desimportantes","title":"2\u00b0 Passo: Remo\u00e7\u00e3o de colunas desimportantes","text":"<p>N\u00e3o h\u00e1 colunas desimportantes para a an\u00e1lise no dataset. Um exemplo de coluna seria um identificador \u00fanico do vinho. Todas s\u00e3o vi\u00e1veis para o modelo de predi\u00e7\u00e3o.</p>"},{"location":"projeto/main/#3-passo-padronizacao-das-features-numericas","title":"3\u00b0 Passo: Padroniza\u00e7\u00e3o das features num\u00e9ricas","text":"<p>Por fim, \u00e9 necess\u00e1rio padronizar as features num\u00e9ricas da base. Ao inv\u00e9s da normaliza\u00e7\u00e3o, ser\u00e1 utilizada a t\u00e9cnica de padroniza\u00e7\u00e3o devido aos outliers nas features num\u00e9ricas. Para a padroniza\u00e7\u00e3o, foi utilkizado o StandardScaler() do <code>scikit-learn</code>.</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX = scaler.fit_transform(df)\n</code></pre>"},{"location":"projeto/main/#etapa-3-k-means","title":"Etapa 3 - K-Means","text":"<p>Nessa etapa, realizaremos um modelo K-Means para clusterizar a base e obter categorias que ser\u00e3o a vari\u00e1vel-alvo da previs\u00e3o dos modelos supervisionados. </p>"},{"location":"projeto/main/#elbow-method","title":"Elbow Method","text":"<p>Antes de treinar o modelo, \u00e9 necess\u00e1rio descobrir o n\u00famero de clusters que ser\u00e1 utilizado. Para isso, aplicaremos o Elbow Method.</p> ElbowC\u00f3digo <p></p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = scaler.fit_transform(df)\n\nwcss = []\nk_range = range(1, 11)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n\nplt.figure(figsize=(10, 6))\nplt.plot(k_range, wcss, \"bo-\", markersize=8, linewidth=2)\nplt.xlabel(\"N\u00famero de Clusters (K)\")\nplt.ylabel(\"WCSS (Within-Cluster Sum of Square)\")\nplt.title(\"Elbow Method - Determinando o K ideal\")\nplt.grid(True, alpha=0.3)\nplt.xticks(k_range)\n\nplt.axvline(x=3, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Poss\u00edvel cotovelo K=3\")\n\nplt.legend()\n# plt.savefig(\"docs/projeto/images/elbow.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre> <p>Podemos observar que o cotovelo est\u00e1 em \\(k = 3\\), logo, esse ser\u00e1 o n\u00famero de clusters utilizado para o K-Means.</p>"},{"location":"projeto/main/#treinamento-do-k-means","title":"Treinamento do K-Means","text":"<p>Para a forma\u00e7\u00e3o dos clusters do K-Means, foi utilizado a t\u00e9cnica do PCA (Principal Component Analysis). </p> K-Means PCAC\u00f3digo <p></p> Silhouette Score: 0.2849 <p></p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = scaler.fit_transform(df)\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nkmeans = KMeans(n_clusters=3, init=\"k-means++\", random_state=42, n_init=10)\ncluster_labels = kmeans.fit_predict(X)\n\nsilhouette_avg = silhouette_score(X, cluster_labels)\nprint(f\"Silhouette Score: {silhouette_avg:.4f}\")\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap=\"viridis\", alpha=0.7)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker=\"*\", s=300, c=\"red\", label=\"Centroids\")\n\nplt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.2%} var.)\")\nplt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.2%} var.)\")\nplt.title(\"Clusters de Vinhos - K-means (K=3)\")\nplt.legend()\nplt.colorbar(scatter, label=\"Cluster\")\n\n# plt.savefig(\"docs/projeto/images/k-means.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre>"},{"location":"projeto/main/#etapa-4-avaliacao-do-k-means","title":"Etapa 4 - Avalia\u00e7\u00e3o do K-Means","text":""},{"location":"projeto/main/#silhouette-score","title":"Silhouette Score","text":"<p>O modelo alcan\u00e7ou um Silhouette Score de 0.2849, indicando uma estrutura de clusters potencialmente artificial. Na escala de -1 a +1, este valor se enquadra na categoria Fraca, por\u00e9m ainda acima do limiar de 0.25 que indicaria aus\u00eancia de clusters naturais.</p>"},{"location":"projeto/main/#variancia-explicada","title":"Vari\u00e2ncia Explicada","text":"<p>O PCA aplicado para visualiza\u00e7\u00e3o explica 55.41% da vari\u00e2ncia total dos dados. Embora seja uma representa\u00e7\u00e3o simplificada, \u00e9 suficiente para identificar padr\u00f5es gerais, por\u00e9m pode n\u00e3o capturar estruturas mais complexas n\u00e3o-lineares.</p>"},{"location":"projeto/main/#conclusao-da-avaliacao","title":"Conclus\u00e3o da avalia\u00e7\u00e3o","text":"<p>O silhouette score ficou baixo, por isso, vamos tentar outra t\u00e9cnica que n\u00e3o o PCA. Vamos explorar o t-SNE (t-Distributed Stochastic Neighbor Embedding), uma t\u00e9cnica n\u00e3o-linear que pode revelar melhor estruturas locais e agrupamentos n\u00e3o capturados pelo PCA.</p>"},{"location":"projeto/main/#re-treinamento-do-k-means","title":"Re-treinamento do k-means","text":"K-Means t-SNEC\u00f3digo Silhouette Score: 0.5928 <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import silhouette_score\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = scaler.fit_transform(df)\n\ntsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\nX_tsne = tsne.fit_transform(X)\n\nkmeans = KMeans(n_clusters=3, init=\"k-means++\", random_state=42, n_init=10)\ncluster_labels = kmeans.fit_predict(X)\n\nsilhouette_avg = silhouette_score(X_tsne, cluster_labels)\nprint(f\"Silhouette Score: {silhouette_avg:.4f}\")\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=cluster_labels, cmap=\"viridis\", alpha=0.7)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n           marker=\"*\", s=300, c=\"red\", label=\"Centroids\")\n\nplt.xlabel(\"t-SNE Component 1\")\nplt.ylabel(\"t-SNE Component 2\")\nplt.title(f\"Clusters de Vinhos - t-SNE + K-means (K=3)\\nSilhouette: {silhouette_avg:.4f}\")\nplt.legend()\nplt.colorbar(scatter, label=\"Cluster\")\n\n# plt.savefig(\"docs/projeto/images/k-means-tsne.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre> <p>Foi poss\u00edvel observar uma melhora significativa no silhouette score, de 0.2849 com PCA para 0.5928 com t-SNE, indicando uma estrutura de clusters muito mais definida.</p> <p>Os centr\u00f3ides na visualiza\u00e7\u00e3o t-SNE podem parecer \"estranhos\" porque esta t\u00e9cnica prioriza a preserva\u00e7\u00e3o de estruturas locais em detrimento de rela\u00e7\u00f5es globais e densidades. O t-SNE distorce intencionalmente o espa\u00e7o para destacar agrupamentos pr\u00f3ximos, o que explica a posi\u00e7\u00e3o n\u00e3o convencional dos centr\u00f3ides na visualiza\u00e7\u00e3o.</p> <p>Em resumo, o trade-off vale a pena: mesmo com a perda de informa\u00e7\u00f5es sobre densidades e estruturas globais, a qualidade da clusteriza\u00e7\u00e3o melhorou drasticamente, revelando padr\u00f5es que n\u00e3o eram aparentes com PCA.</p>"},{"location":"projeto/main/#criando-a-coluna-da-variavel-alvo","title":"Criando a coluna da vari\u00e1vel-alvo","text":"<p>Agora, vamos criar a coluna que conter\u00e1 a vari\u00e1vel categ\u00f3rica <code>Wine_Type</code>, criada a partir da clusteriza\u00e7\u00e3o do K-Means com t-SNE. Essa coluna ser\u00e1 a vari\u00e1vel-alvo das an\u00e1lises preditivas feitas pelos modelos supervisionados adiante.</p> Sa\u00eddaC\u00f3digo <p>Distribui\u00e7\u00e3o dos clusters - Wine Type 1: 65 Wine Type 2: 51 Wine Type 3: 62</p> <pre><code>import pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = scaler.fit_transform(df)\n\ntsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\nX_tsne = tsne.fit_transform(X)\n\nkmeans = KMeans(n_clusters=3, init=\"k-means++\", random_state=42, n_init=10)\ncluster_labels = kmeans.fit_predict(X)\n\ndf[\"cluster\"] = cluster_labels\ndf[\"Wine_Type\"] = [f\"Wine Type {label + 1}\" for label in cluster_labels]\n\nprint(\"Distribui\u00e7\u00e3o dos clusters -&lt;br&gt;\")\nfor i in range(1, 4):\n    count = len(df[df['Wine_Type'] == f'Wine Type {i}'])\n    print(f\"Wine Type {i}: {count}&lt;br&gt;\")\n\n# Salvar para csv\n# df.to_csv(\"wine-final.csv\", index=False)\n</code></pre> <p>Com isso, podemos partir para as pr\u00f3ximas etapas.</p>"},{"location":"projeto/main/#etapa-5-divisao-de-dados","title":"Etapa 5 - Divis\u00e3o de dados","text":"<p>Em seguida, vamos realizar a divis\u00e3o dos dados em conjuntos de treino e teste.</p> <ul> <li> <p>Conjunto de Treino: Utilizado para ensinar o modelo a reconhecer padr\u00f5es</p> </li> <li> <p>Conjunto de Teste: Utilizado para avaliar o desempenho do modelo com dados ainda n\u00e3o vistos</p> </li> </ul> <p>Para realizar a divis\u00e3o, foi utilizada a fun\u00e7\u00e3o train_test_split() do <code>scikit-learn</code>. Os par\u00e2metros utilizados s\u00e3o:</p> <ul> <li> <p>test_size=0.2: Define que 20% dos dados ser\u00e3o utilizados para teste, enquanto o restante ser\u00e1 usado para treino.</p> </li> <li> <p>random_state=42: Par\u00e2metro que controla o gerador de n\u00famero aleat\u00f3rios utilizado para sortear os dados antes de separ\u00e1-los. Garante reprodutibilidade.</p> </li> <li> <p>stratify=y: Esse atributo definido como y \u00e9 essencial devido \u00e0 natureza da coluna <code>Wine_Type</code>. Com essa defini\u00e7\u00e3o, ser\u00e1 mantida a mesma propor\u00e7\u00e3o das categorias em ambos os conjuntos, reduzindo o vi\u00e9s.</p> </li> </ul> Sa\u00eddaC\u00f3digo <p>Treino: 142 amostras</p> <p>Teste: 36 amostras</p> <p>Propor\u00e7\u00e3o: 79.8% treino, 20.2% teste</p> <p>Distribui\u00e7\u00e3o das classes - </p> <p>Treino:</p> Wine_Type count Wine Type 1 52 Wine Type 3 49 Wine Type 2 41 <p>Teste:</p> Wine_Type count Wine Type 3 13 Wine Type 1 13 Wine Type 2 10 <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(\"Wine_Type\", axis=1)\nX = scaler.fit_transform(X)\ny = df[\"Wine_Type\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(f\"Treino: {X_train.shape[0]} amostras\\n\")\nprint(f\"Teste: {X_test.shape[0]} amostras\\n\")\nprint(f\"Propor\u00e7\u00e3o: {X_train.shape[0]/X.shape[0]*100:.1f}% treino, {X_test.shape[0]/X.shape[0]*100:.1f}% teste\\n\")\n\nprint(\"Distribui\u00e7\u00e3o das classes - \\n\")\nprint(\"Treino:\\n\")\nprint(y_train.value_counts().to_markdown(), \"\\n\")\nprint(\"Teste:\\n\")\nprint(y_test.value_counts().to_markdown(), \"\\n\")\n</code></pre> <p>Esta divis\u00e3o adequada \u00e9 de extrema import\u00e2ncia, pois ajuda a evitar overfitting.</p>"},{"location":"projeto/main/#etapa-6-treinamento-do-modelo-decision-tree","title":"Etapa 6 - Treinamento do modelo Decision Tree","text":"<p>Agora, vamos treinar um modelo de \u00e1rvore de decis\u00f5es (Decision Tree) para prever a vari\u00e1vel alvo <code>Wine_Type</code> para os dados do conjunto teste. Nosso objetivo aqui \u00e9 treinar e avaliar o modelo, para depois compar\u00e1-lo ao KNN e decidir o melhor para este caso.</p> Decision TreeC\u00f3digo <p></p> <p></p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import tree\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\nX = scaler.fit_transform(X)\ny = df[\"Wine_Type\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nclassifier = tree.DecisionTreeClassifier(random_state=42)\nclassifier.fit(X_train, y_train)\n\nplt.figure(figsize=(12,9))\ntree.plot_tree(\n    classifier,\n    feature_names=pd.DataFrame(X).columns,\n    class_names=classifier.classes_,\n    filled=True,\n    rounded=True,\n    max_depth=3,\n    fontsize=10\n)\nplt.title(\"\u00c1rvore de Decis\u00e3o - Faixas de \u00c1lcool (Vinhos)\")\n\n# plt.savefig(\"docs/projeto/images/d-tree.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre>"},{"location":"projeto/main/#etapa-7-avaliacao-do-modelo-decision-tree","title":"Etapa 7 - Avalia\u00e7\u00e3o do modelo Decision Tree","text":"<p>Agora, vamos realizar a avalia\u00e7\u00e3o do modelo treinado. Primeiramente, vamos ver a acur\u00e1cia do modelo e a import\u00e2ncia de cada uma das features utilizadas para a predi\u00e7\u00e3o.</p> Sa\u00eddaC\u00f3digo <p>Acur\u00e1cia do Modelo: 0.8889 Import\u00e2ncia das Features:  Feature Import\u00e2ncia 12 Proline 0.427374 11 OD280 0.359550 6 Flavanoids 0.113809 9 Color_Intensity 0.076206 1 Malic_Acid 0.018196 0 Alcohol 0.004865 4 Magnesium 0.000000 2 Ash 0.000000 3 Ash_Alcanity 0.000000 8 Proanthocyanins 0.000000 7 Nonflavanoid_Phenols 0.000000 5 Total_Phenols 0.000000 10 Hue 0.000000 </p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import tree\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\nX = scaler.fit_transform(X)\ny = df[\"Wine_Type\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nclassifier = tree.DecisionTreeClassifier(random_state=42)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Acur\u00e1cia do Modelo: {accuracy:.4f}\")\n\nfeature_names = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1).columns\n\nfeature_importance = pd.DataFrame({\n    \"Feature\": feature_names,\n    \"Import\u00e2ncia\": classifier.feature_importances_\n})\nprint(\"&lt;br&gt;Import\u00e2ncia das Features:\")\nprint(feature_importance.sort_values(by=\"Import\u00e2ncia\", ascending=False).to_html() + \"&lt;br&gt;\")\n</code></pre>"},{"location":"projeto/main/#acuracia","title":"Acur\u00e1cia","text":"<p>O modelo atingiu uma boa acur\u00e1cia, de 88,89%, bem pr\u00f3ximo do ideal de 95%. Isso significa que, em 88,89% das previs\u00f5es feitas, o tipo de vinho predito est\u00e1 correto.</p>"},{"location":"projeto/main/#importancia-das-features","title":"Import\u00e2ncia das features","text":"<ul> <li> <p>Na tabela de import\u00e2ncia das features, podemos notar que a vari\u00e1vel mais importante para a previs\u00e3o \u00e9 a <code>Proline</code>, com 42,74% de import\u00e2ncia na previs\u00e3o. </p> </li> <li> <p>Diversas vari\u00e1veis tiveram uma import\u00e2ncia nula, sendo elas: <code>Magnesium</code>, <code>Ash</code>, <code>Ash_Alcanity</code>, <code>Proanthocyanins</code>, <code>Nonflavanoid_Phenols</code>, <code>Total_Phenols</code> e <code>Hue</code></p> </li> <li> <p>As vari\u00e1veis <code>Malic_Acid</code> e <code>Alcohol</code> tiveram uma import\u00e2ncia quase irrelevante na predi\u00e7\u00e3o, de 1,82% e 0,49% respectivamente.</p> </li> </ul>"},{"location":"projeto/main/#matriz-de-confusao","title":"Matriz de Confus\u00e3o","text":"<p>Agora, vamos visualizar a matriz de confus\u00e3o do modelo.</p> Sa\u00eddaC\u00f3digo <p>Matriz de confus\u00e3o</p> <p></p> <p>M\u00e9tricas de qualidade</p> precision recall f1-score support Wine Type 1 0.8 0.92 0.86 13 Wine Type 2 0.9 0.9 0.9 10 Wine Type 3 1 0.85 0.92 13 accuracy 0.89 0.89 0.89 0.89 macro avg 0.9 0.89 0.89 36 weighted avg 0.9 0.89 0.89 36 <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn import tree\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\nX = scaler.fit_transform(X)\ny = df[\"Wine_Type\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nclassifier = tree.DecisionTreeClassifier(random_state=42)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)\ndisp.plot(cmap=\"Blues\")\nplt.title(\"Matriz de Confus\u00e3o - Wine\")\n# plt.savefig(\"docs/projeto/images/cm-d-tree.svg\", format=\"svg\", transparent=True)\nplt.close()\n\nreport_dict = classification_report(y_test, y_pred, output_dict=True)\nreport_df = pd.DataFrame(report_dict).transpose()\n\nprint(report_df.round(2).to_markdown())\n</code></pre>"},{"location":"projeto/main/#avaliacao-das-metricas","title":"Avalia\u00e7\u00e3o das m\u00e9tricas","text":"<p>Pontos Positivos</p> <ul> <li> <p>Boa acur\u00e1cia geral: 89% - modelo consegue classificar corretamente a maioria das inst\u00e2ncias</p> </li> <li> <p>Excelente precis\u00e3o para Wine Type 3: 100% - quando o modelo classifica como tipo 3, est\u00e1 sempre correto</p> </li> <li> <p>Recall alto para Wine Type 1: 92% - consegue identificar quase todos os vinhos do tipo 1</p> </li> <li> <p>Balanceamento razo\u00e1vel: M\u00e9tricas similares entre as classes</p> </li> </ul> <p>Pontos de Melhoria</p> <p>Problema com Wine Type 3:</p> <ul> <li>Recall de 85% - o modelo falha em identificar 15% dos vinhos do tipo 3</li> </ul> <p>Isso significa que 15% dos vinhos tipo 3 est\u00e3o sendo classificados erroneamente como outros tipos</p> <p>Precis\u00e3o do Wine Type 1:</p> <ul> <li>80% - quando o modelo diz \"\u00e9 tipo 1\", em 20% dos casos est\u00e1 errado</li> </ul>"},{"location":"projeto/main/#etapa-8-treinamento-do-modelo-knn","title":"Etapa 8 - Treinamento do Modelo KNN","text":"<p>Agora, vamos treinar um modelo de KNN para prever a vari\u00e1vel alvo <code>Wine_Type</code> para os dados do conjunto teste. Nosso objetivo aqui \u00e9 treinar e avaliar o modelo, para depois compar\u00e1-lo ao modelo de \u00e1rvore de decis\u00f5es e apontar o modelo superior para este caso.</p> KNNC\u00f3digo <p></p> Acur\u00e1cia: 0.9722  <p></p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\nX = scaler.fit_transform(X)\ny = df[\"Wine_Type\"]\n\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions):.4f}&lt;br&gt;\")\n\n# Visualiza\u00e7\u00e3o do KNN \n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nprint(\"Variance explained by each component:\", pca.explained_variance_ratio_)\n\nX_train_pca, X_test_pca, _, _ = train_test_split(X_pca, y_encoded, test_size=0.2, random_state=42)\nknn_pca = KNeighborsClassifier(n_neighbors=3)\nknn_pca.fit(X_train_pca, y_train)\n\nh = .05\nx_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\ny_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\nnp.arange(y_min, y_max, h))\n\nZ = knn_pca.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.figure(figsize=(10, 7))\nplt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\nsns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette=\"coolwarm\", edgecolor=\"k\", s=60)\nplt.title(\"KNN com PCA do Modelo 1)\")\nplt.xlabel(\"Componente Principal 1\")\nplt.ylabel(\"Componente Principal 2\")\nplt.legend(title=\"Booking status\")\n\n# plt.savefig(\"docs/projeto/images/knn.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre>"},{"location":"projeto/main/#etapa-9-avaliacao-do-modelo-knn","title":"Etapa 9 - Avalia\u00e7\u00e3o do modelo KNN","text":"<p>Agora, vamos realizar a avalia\u00e7\u00e3o do modelo KNN.</p>"},{"location":"projeto/main/#acuracia_1","title":"Acur\u00e1cia","text":"<p>O modelo alcan\u00e7ou uma acur\u00e1cia de 97,22%, que \u00e9 excelente, contudo indica poss\u00edvel overfitting no modelo. Para testar essa hip\u00f3tese, vamos fazer um teste de acur\u00e1cia nos conjuntos de treino e teste separadamente com KNN e uma valida\u00e7\u00e3o cruzada.</p>"},{"location":"projeto/main/#acuracias-dos-conjuntos-e-validacao-cruzada","title":"Acur\u00e1cias dos conjuntos e valida\u00e7\u00e3o cruzada","text":"Testes overfittingC\u00f3digo <p>Acur\u00e1cias dos conjuntos -</p> <p>Acur\u00e1cia no Treino: 0.9718 </p> <p>Acur\u00e1cia no Teste: 0.9722</p> <p>Valida\u00e7\u00e3o Cruzada (5-fold) -</p> <p>Scores: [0.88888889 0.97222222 0.97222222 1.         0.94285714]</p> <p>M\u00e9dia: 0.9552 (+/- 0.0756)</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.decomposition import PCA\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\nX = scaler.fit_transform(X)\ny = df[\"Wine_Type\"]\n\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nX_train_pca, X_test_pca, _, _ = train_test_split(X_pca, y_encoded, test_size=0.2, random_state=42)\nknn_pca = KNeighborsClassifier(n_neighbors=3)\nknn_pca.fit(X_train_pca, y_train)\n\ntrain_accuracy = knn.score(X_train, y_train)\ntest_accuracy = knn.score(X_test, y_test)\nprint(f\"\\n&lt;b&gt;Acur\u00e1cias dos conjuntos -&lt;/b&gt;\\n\")\nprint(f\"Acur\u00e1cia no Treino: {train_accuracy:.4f} \\n\")\nprint(f\"Acur\u00e1cia no Teste: {test_accuracy:.4f}\")\n\ncv_scores = cross_val_score(knn, X, y_encoded, cv=5)\nprint(f\"\\n&lt;b&gt;Valida\u00e7\u00e3o Cruzada (5-fold) -&lt;/b&gt;\\n\")\nprint(f\"Scores: {cv_scores}\\n\")\nprint(f\"M\u00e9dia: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n</code></pre> <p>Com esses resultados, podemos concluir que h\u00e1 muita chance desse n\u00e3o ser um caso de overfitting. Isso porque as acur\u00e1cias dos conjuntos s\u00e3o consistentes, variando apenas em 0,04%. Al\u00e9m disso, a valida\u00e7\u00e3o cruzada nos demonstrou uma alta m\u00e9dia, de 95,52%, um desvio padr\u00e3o baixo (aproximadamente 3,78%) e uma varia\u00e7\u00e3o dos scores entre 88,9% \u00e0 100%, uma varia\u00e7\u00e3o normal.</p>"},{"location":"projeto/main/#matriz-de-confusao_1","title":"Matriz de Confus\u00e3o","text":"Matriz de Confus\u00e3oC\u00f3digo <p>Matriz de confus\u00e3o</p> <p></p> <p>M\u00e9tricas de qualidade</p> precision recall f1-score support 0 1 0.92 0.96 13 1 1 1 1 9 2 0.93 1 0.97 14 accuracy 0.97 0.97 0.97 0.97 macro avg 0.98 0.97 0.98 36 weighted avg 0.97 0.97 0.97 36 <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\nX = scaler.fit_transform(X)\ny = df[\"Wine_Type\"]\n\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\n\ncm = confusion_matrix(y_test, predictions)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\nplt.xlabel(\"Predito\")\nplt.ylabel(\"Real\")\nplt.title(\"Matriz de Confus\u00e3o - KNN\")\n\n# plt.savefig(\"docs/projeto/images/cm-knn.svg\", format=\"svg\", transparent=True)\nplt.close()\n\nreport_dict = classification_report(y_test, predictions, output_dict=True)\nreport_df = pd.DataFrame(report_dict).transpose()\n\nprint(report_df.round(2).to_markdown())\n</code></pre> <p>O modelo atingiu uma performance excepcional, com acur\u00e1cia geral de 97%, classe 1 perfeitamente prevista pelo modelo com Precis\u00e3o, Recall e F1-Score de 1.00 e alta consist\u00eancia geral, j\u00e1 que todas classes possuem F1-Score acima de 0.96.</p>"},{"location":"projeto/main/#etapa-10-relatorio-final","title":"Etapa 10 - Relat\u00f3rio Final","text":"<p>Ap\u00f3s extensa an\u00e1lise comparativa dos modelos desenvolvidos para a classifica\u00e7\u00e3o de vinhos, o algoritmo K-Nearest Neighbors (KNN) emergiu como a escolha ideal para este problema preditivo, demonstrando performance excepcionalmente superior em todas as m\u00e9tricas de avalia\u00e7\u00e3o. </p> <p>A estrat\u00e9gia de clusteriza\u00e7\u00e3o com K-Means utilizando visualiza\u00e7\u00e3o t-SNE provou-se notavelmente superior \u00e0 abordagem com PCA, oferecendo:</p> <ul> <li> <p>Separa\u00e7\u00e3o mais n\u00edtida entre os clusters de vinhos</p> </li> <li> <p>Preserva\u00e7\u00e3o superior das estruturas locais dos dados</p> </li> <li> <p>Visualiza\u00e7\u00e3o mais intuitiva das rela\u00e7\u00f5es entre as variedades</p> </li> <li> <p>Agrupamentos mais coesos e semanticamente significativos</p> </li> </ul> <p>O t-SNE demonstrou boa capacidade em revelar a estrutura subjacente do dataset, permitindo identificar grupos naturais de vinhos que se alinham perfeitamente com suas caracter\u00edsticas intr\u00ednsecas e qualidade.</p> <p>Embora o K-Means com t-SNE tenha demonstrado resultados promissores, o Silhouette Score de 0.5928 indica espa\u00e7o para otimiza\u00e7\u00e3o, sugerindo que a separa\u00e7\u00e3o entre clusters pode ser aprimorada atrav\u00e9s de outras t\u00e9cnicas como DBSCAN, al\u00e9m do refinamento do pr\u00e9-processamento dos dados e experimenta\u00e7\u00e3o com diferentes redu\u00e7\u00f5es de dimensionalidade.</p>"}]}
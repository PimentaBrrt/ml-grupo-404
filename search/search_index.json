{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#grupo-404-name_not_found","title":"Grupo 404 - Name_Not_Found","text":""},{"location":"#integrantes","title":"Integrantes","text":"<ol> <li>Guilherme  Orlandi de Oliveira</li> <li>Luis Felipe Galina Degaspari</li> <li>Luiz Felipe Pimenta Berrettini</li> <li>Luiz Fernando Pazdziora Costa</li> </ol> <p>Estudantes do 4\u00b0 semestre de Ci\u00eancias de Dados e Neg\u00f3cios (CDN) na ESPM (Escola Superior de Propaganda e Marketing). </p> <p>Projetos de machine learning realizados em 2025.2, orientados e supervisionados pelo professor Humberto Sandmann.</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Projeto 1 - Data 30/09/2025</li> <li> Projeto 2 - Data 05/12/2025</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"projeto/main/","title":"Projeto 1","text":""},{"location":"projeto/main/#modelo-de-machine-learning-arvore-de-decisoes-knn-e-k-means","title":"Modelo de Machine Learning - \u00c1rvore de Decis\u00f5es, KNN e K-Means","text":"<p>Para esse projeto, foi utilizado um dataset obtido no Kaggle. Os dados usados podem ser baixados aqui, e foram adaptados de outra base de vinhos.</p>"},{"location":"projeto/main/#objetivo","title":"Objetivo","text":"<p>O dataset possui informa\u00e7\u00f5es diversas sobre resultados de an\u00e1lises qu\u00edmicas de vinhos produzidos na mesma regi\u00e3o da It\u00e1lia, mas derivados de tr\u00eas diferentes cultivares. A an\u00e1lise determinou as quantidade de 13 constituintes encontrados em cada um dos tr\u00eas tipos de vinho. O objetivo da an\u00e1lise \u00e9 clusterizar a base atrav\u00e9s do k-means e, com os modelos supervisionados, prever o tipo de vinho com base nos dados fornecidos.</p>"},{"location":"projeto/main/#workflow","title":"Workflow","text":"<p>Os pontos \"etapas\" s\u00e3o o passo-a-passo da realiza\u00e7\u00e3o do projeto.</p>"},{"location":"projeto/main/#etapa-1-exploracao-de-dados","title":"Etapa 1 - Explora\u00e7\u00e3o de Dados","text":"<p>Primeiramente, para entender melhor a base de dados, vamos descobrir quantas linhas e colunas o dataset possui.</p> Sa\u00eddaC\u00f3digo <p>(178, 13)</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nprint(df.shape)\n</code></pre> <p>Como foi poss\u00edvel observar no c\u00f3digo acima, o dataset possui 178 linhas e 13 colunas, com cada linha possuindo os dados de um vinho.</p>"},{"location":"projeto/main/#colunas-do-dataset","title":"Colunas do Dataset","text":"<p>Em seguida, \u00e9 necess\u00e1rio descobrir a natureza dos dados. Isso ser\u00e1 feito rodando as linhas de c\u00f3digo abaixo:</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nprint(df.info())\n</code></pre> <p>As informa\u00e7\u00f5es obtidas foram as seguintes:</p> Coluna Tipo Descri\u00e7\u00e3o <code>Alcohol</code> Float Teor alco\u00f3lico do vinho <code>Malic_Acid</code> Float \u00c1cido m\u00e1lico <code>Ash</code> Float Cinzas <code>Ash_Alcanity</code> Float Alcalinidade das cinzas <code>Magnesium</code> Inteiro Magn\u00e9sio <code>Total_Phenols</code> Float Total de fen\u00f3is <code>Flavanoids</code> Float Flavan\u00f3ides <code>Nonflavanoid_Phenols</code> Float Fen\u00f3is n\u00e3o-flavan\u00f3ides <code>Proanthocyanins</code> Float Proantocianidinas <code>Color_Intensity</code> Float Intensidade da cor do vinho <code>Hue</code> Float Satura\u00e7\u00e3o do vinho <code>OD280</code> Float Rela\u00e7\u00e3o OD280/OD315 de vinhos dilu\u00eddos <code>Proline</code> Inteiro Prolina"},{"location":"projeto/main/#visualizacao-das-variaveis","title":"Visualiza\u00e7\u00e3o das vari\u00e1veis","text":"<p>Em seguida, \u00e9 essencial realizar gr\u00e1ficos para visualizar como cada uma das vari\u00e1veis se comportam, com o objetivo de entender melhor a base da dados. Todas vari\u00e1veis da base s\u00e3o quantitativas, sendo onze cont\u00ednuas e duas discretas.</p>"},{"location":"projeto/main/#variaveis-quantitativas-continuas","title":"Vari\u00e1veis Quantitativas Cont\u00ednuas","text":"<p>Para cada uma das vari\u00e1veis num\u00e9ricas cont\u00ednuas, ser\u00e1 feito um histograma com o objetivo de visualizar a frequ\u00eancia de valores.</p> AlcoholMalic_AcidAshAsh_AlcanityTotal_PhenolsFlavanoidsNonflavanoid_PhenolsProanthocyaninsColor_IntensityHueOD280 Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:02.518280 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Alcohol\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"red\")\nplt.title(\"Distribui\u00e7\u00e3o do Teor Alco\u00f3lico dos Vinhos - Histograma\")\nplt.xlabel(\"Teor Alco\u00f3lico\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:02.731655 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Malic_Acid\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"yellow\")\nplt.title(\"Distribui\u00e7\u00e3o de \u00c1cido M\u00e1lico dos Vinhos - Histograma\")\nplt.xlabel(\"\u00c1cido M\u00e1lico\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:02.908936 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Ash\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"green\")\nplt.title(\"Distribui\u00e7\u00e3o de Cinzas dos Vinhos - Histograma\")\nplt.xlabel(\"Cinzas\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:03.192703 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Ash_Alcanity\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"blue\")\nplt.title(\"Distribui\u00e7\u00e3o da Alcalinidade das Cinzas dos Vinhos - Histograma\")\nplt.xlabel(\"Alcalinidade das Cinzas\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:03.368372 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Total_Phenols\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"purple\")\nplt.title(\"Distribui\u00e7\u00e3o do Total de Fen\u00f3is dos Vinhos - Histograma\")\nplt.xlabel(\"Total de Fen\u00f3is\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:03.526998 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Flavanoids\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"orange\")\nplt.title(\"Distribui\u00e7\u00e3o de Flavan\u00f3ides dos Vinhos - Histograma\")\nplt.xlabel(\"Flavan\u00f3ides\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:03.685961 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Nonflavanoid_Phenols\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"lightblue\")\nplt.title(\"Distribui\u00e7\u00e3o de Fen\u00f3is n\u00e3o Flavan\u00f3ides dos Vinhos - Histograma\")\nplt.xlabel(\"Fen\u00f3is n\u00e3o Flavan\u00f3ides\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:03.835648 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Proanthocyanins\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"brown\")\nplt.title(\"Distribui\u00e7\u00e3o de Proantocianidinas dos Vinhos - Histograma\")\nplt.xlabel(\"Proantocianidinas\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:03.999994 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Color_Intensity\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"grey\")\nplt.title(\"Distribui\u00e7\u00e3o de Intensidade da Cor dos Vinhos - Histograma\")\nplt.xlabel(\"Intensidade da Cor\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:04.155836 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Hue\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"lightgreen\")\nplt.title(\"Distribui\u00e7\u00e3o de Satura\u00e7\u00e3o dos Vinhos - Histograma\")\nplt.xlabel(\"Satura\u00e7\u00e3o\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:04.315308 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"OD280\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"pink\")\nplt.title(\"Distribui\u00e7\u00e3o do OD280 dos Vinhos - Histograma\")\nplt.xlabel(\"OD280\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre>"},{"location":"projeto/main/#variaveis-quantitativas-discretas","title":"Vari\u00e1veis Quantitativas Discretas","text":"<p>Para ambas vari\u00e1veis num\u00e9ricas discretas, tamb\u00e9m faremos histogramas.</p> MagnesiumProline Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:04.485124 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Magnesium\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"lightgreen\")\nplt.title(\"Distribui\u00e7\u00e3o de Magn\u00e9sio dos Vinhos - Histograma\")\nplt.xlabel(\"Magn\u00e9sio\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:04.645532 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Proline\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"skyblue\")\nplt.title(\"Distribui\u00e7\u00e3o de Prolina dos Vinhos - Histograma\")\nplt.xlabel(\"Prolina\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> <p>Atrav\u00e9s das an\u00e1lises, foi poss\u00edvel alcan\u00e7ar uma compreens\u00e3o mais aprofundada do funcionamento de cada uma das vari\u00e1veis no dataset, al\u00e9m de haver insights valiosos nesses gr\u00e1ficos.</p>"},{"location":"projeto/main/#etapa-2-pre-processamento","title":"Etapa 2 - Pr\u00e9-processamento","text":""},{"location":"projeto/main/#1-passo-identificacao-de-valores-nulos","title":"1\u00b0 Passo: Identifica\u00e7\u00e3o de valores nulos","text":"<p>Atrav\u00e9s da linha de c\u00f3digo abaixo, pode-se identificar que n\u00e3o h\u00e1 valores nulos na base. Portanto, pularemos o passo de tratamento de valores nulos.</p> <pre><code>print(df.isnull().sum())\n</code></pre>"},{"location":"projeto/main/#2-passo-remocao-de-colunas-desimportantes","title":"2\u00b0 Passo: Remo\u00e7\u00e3o de colunas desimportantes","text":"<p>N\u00e3o h\u00e1 colunas desimportantes para a an\u00e1lise no dataset. Um exemplo de coluna seria um identificador \u00fanico do vinho. Todas s\u00e3o vi\u00e1veis para o modelo de predi\u00e7\u00e3o.</p>"},{"location":"projeto/main/#3-passo-padronizacao-das-features-numericas","title":"3\u00b0 Passo: Padroniza\u00e7\u00e3o das features num\u00e9ricas","text":"<p>Por fim, \u00e9 necess\u00e1rio padronizar as features num\u00e9ricas da base. Ao inv\u00e9s da normaliza\u00e7\u00e3o, ser\u00e1 utilizada a t\u00e9cnica de padroniza\u00e7\u00e3o devido aos outliers nas features num\u00e9ricas. Para a padroniza\u00e7\u00e3o, foi utilkizado o StandardScaler() do <code>scikit-learn</code>.</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX = scaler.fit_transform(df)\n</code></pre>"},{"location":"projeto/main/#etapa-3-k-means","title":"Etapa 3 - K-Means","text":"<p>Nessa etapa, realizaremos um modelo K-Means para clusterizar a base e obter categorias que ser\u00e3o a vari\u00e1vel-alvo da previs\u00e3o dos modelos supervisionados. </p>"},{"location":"projeto/main/#elbow-method","title":"Elbow Method","text":"<p>Antes de treinar o modelo, \u00e9 necess\u00e1rio descobrir o n\u00famero de clusters que ser\u00e1 utilizado. Para isso, aplicaremos o Elbow Method.</p> ElbowC\u00f3digo <p></p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = scaler.fit_transform(df)\n\nwcss = []\nk_range = range(1, 11)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n\nplt.figure(figsize=(10, 6))\nplt.plot(k_range, wcss, \"bo-\", markersize=8, linewidth=2)\nplt.xlabel(\"N\u00famero de Clusters (K)\")\nplt.ylabel(\"WCSS (Within-Cluster Sum of Square)\")\nplt.title(\"Elbow Method - Determinando o K ideal\")\nplt.grid(True, alpha=0.3)\nplt.xticks(k_range)\n\nplt.axvline(x=3, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Poss\u00edvel cotovelo K=3\")\n\nplt.legend()\n# plt.savefig(\"docs/projeto/images/elbow.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre> <p>Podemos observar que o cotovelo est\u00e1 em \\(k = 3\\), logo, esse ser\u00e1 o n\u00famero de clusters utilizado para o K-Means.</p>"},{"location":"projeto/main/#treinamento-do-k-means","title":"Treinamento do K-Means","text":"<p>Para a forma\u00e7\u00e3o dos clusters do K-Means, foi utilizado a t\u00e9cnica do PCA (Principal Component Analysis). </p> K-Means PCAC\u00f3digo <p></p> Silhouette Score: 0.2849 <p></p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = scaler.fit_transform(df)\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nkmeans = KMeans(n_clusters=3, init=\"k-means++\", random_state=42, n_init=10)\ncluster_labels = kmeans.fit_predict(X)\n\nsilhouette_avg = silhouette_score(X, cluster_labels)\nprint(f\"Silhouette Score: {silhouette_avg:.4f}\")\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap=\"viridis\", alpha=0.7)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker=\"*\", s=300, c=\"red\", label=\"Centroids\")\n\nplt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.2%} var.)\")\nplt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.2%} var.)\")\nplt.title(\"Clusters de Vinhos - K-means (K=3)\")\nplt.legend()\nplt.colorbar(scatter, label=\"Cluster\")\n\n# plt.savefig(\"docs/projeto/images/k-means.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre>"},{"location":"projeto/main/#etapa-4-avaliacao-do-k-means","title":"Etapa 4 - Avalia\u00e7\u00e3o do K-Means","text":""},{"location":"projeto/main/#silhouette-score","title":"Silhouette Score","text":"<p>O modelo alcan\u00e7ou um Silhouette Score de 0.2849, indicando uma estrutura de clusters potencialmente artificial. Na escala de -1 a +1, este valor se enquadra na categoria Fraca, por\u00e9m ainda acima do limiar de 0.25 que indicaria aus\u00eancia de clusters naturais.</p>"},{"location":"projeto/main/#variancia-explicada","title":"Vari\u00e2ncia Explicada","text":"<p>O PCA aplicado para visualiza\u00e7\u00e3o explica 55.41% da vari\u00e2ncia total dos dados. Embora seja uma representa\u00e7\u00e3o simplificada, \u00e9 suficiente para identificar padr\u00f5es gerais, por\u00e9m pode n\u00e3o capturar estruturas mais complexas n\u00e3o-lineares.</p>"},{"location":"projeto/main/#conclusao-da-avaliacao","title":"Conclus\u00e3o da avalia\u00e7\u00e3o","text":"<p>O silhouette score ficou baixo, por isso, vamos tentar outra t\u00e9cnica que n\u00e3o o PCA. Vamos explorar o t-SNE (t-Distributed Stochastic Neighbor Embedding), uma t\u00e9cnica n\u00e3o-linear que pode revelar melhor estruturas locais e agrupamentos n\u00e3o capturados pelo PCA.</p>"},{"location":"projeto/main/#re-treinamento-do-k-means","title":"Re-treinamento do k-means","text":"K-Means t-SNEC\u00f3digo Silhouette Score: 0.5928 <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import silhouette_score\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = scaler.fit_transform(df)\n\ntsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\nX_tsne = tsne.fit_transform(X)\n\nkmeans = KMeans(n_clusters=3, init=\"k-means++\", random_state=42, n_init=10)\ncluster_labels = kmeans.fit_predict(X)\n\nsilhouette_avg = silhouette_score(X_tsne, cluster_labels)\nprint(f\"Silhouette Score: {silhouette_avg:.4f}\")\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=cluster_labels, cmap=\"viridis\", alpha=0.7)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n           marker=\"*\", s=300, c=\"red\", label=\"Centroids\")\n\nplt.xlabel(\"t-SNE Component 1\")\nplt.ylabel(\"t-SNE Component 2\")\nplt.title(f\"Clusters de Vinhos - t-SNE + K-means (K=3)\\nSilhouette: {silhouette_avg:.4f}\")\nplt.legend()\nplt.colorbar(scatter, label=\"Cluster\")\n\n# plt.savefig(\"docs/projeto/images/k-means-tsne.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre> <p>Foi poss\u00edvel observar uma melhora significativa no silhouette score, de 0.2849 com PCA para 0.5928 com t-SNE, indicando uma estrutura de clusters muito mais definida.</p> <p>Os centr\u00f3ides na visualiza\u00e7\u00e3o t-SNE podem parecer \"estranhos\" porque esta t\u00e9cnica prioriza a preserva\u00e7\u00e3o de estruturas locais em detrimento de rela\u00e7\u00f5es globais e densidades. O t-SNE distorce intencionalmente o espa\u00e7o para destacar agrupamentos pr\u00f3ximos, o que explica a posi\u00e7\u00e3o n\u00e3o convencional dos centr\u00f3ides na visualiza\u00e7\u00e3o.</p> <p>Em resumo, o trade-off vale a pena: mesmo com a perda de informa\u00e7\u00f5es sobre densidades e estruturas globais, a qualidade da clusteriza\u00e7\u00e3o melhorou drasticamente, revelando padr\u00f5es que n\u00e3o eram aparentes com PCA.</p>"},{"location":"projeto/main/#criando-a-coluna-da-variavel-alvo","title":"Criando a coluna da vari\u00e1vel-alvo","text":"<p>Agora, vamos criar a coluna que conter\u00e1 a vari\u00e1vel categ\u00f3rica <code>Wine_Type</code>, criada a partir da clusteriza\u00e7\u00e3o do K-Means com t-SNE. Essa coluna ser\u00e1 a vari\u00e1vel-alvo das an\u00e1lises preditivas feitas pelos modelos supervisionados adiante.</p> Sa\u00eddaC\u00f3digo <p>Distribui\u00e7\u00e3o dos clusters - Wine Type 1: 65 Wine Type 2: 51 Wine Type 3: 62</p> <pre><code>import pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = scaler.fit_transform(df)\n\ntsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\nX_tsne = tsne.fit_transform(X)\n\nkmeans = KMeans(n_clusters=3, init=\"k-means++\", random_state=42, n_init=10)\ncluster_labels = kmeans.fit_predict(X)\n\ndf[\"cluster\"] = cluster_labels\ndf[\"Wine_Type\"] = [f\"Wine Type {label + 1}\" for label in cluster_labels]\n\nprint(\"Distribui\u00e7\u00e3o dos clusters -&lt;br&gt;\")\nfor i in range(1, 4):\n    count = len(df[df['Wine_Type'] == f'Wine Type {i}'])\n    print(f\"Wine Type {i}: {count}&lt;br&gt;\")\n\n# Salvar para csv\n# df.to_csv(\"wine-final.csv\", index=False)\n</code></pre> <p>Com isso, podemos partir para as pr\u00f3ximas etapas.</p>"},{"location":"projeto/main/#etapa-5-divisao-de-dados","title":"Etapa 5 - Divis\u00e3o de dados","text":"<p>Em seguida, vamos realizar a divis\u00e3o dos dados em conjuntos de treino e teste.</p> <ul> <li> <p>Conjunto de Treino: Utilizado para ensinar o modelo a reconhecer padr\u00f5es</p> </li> <li> <p>Conjunto de Teste: Utilizado para avaliar o desempenho do modelo com dados ainda n\u00e3o vistos</p> </li> </ul> <p>Para realizar a divis\u00e3o, foi utilizada a fun\u00e7\u00e3o train_test_split() do <code>scikit-learn</code>. Os par\u00e2metros utilizados s\u00e3o:</p> <ul> <li> <p>test_size=0.2: Define que 20% dos dados ser\u00e3o utilizados para teste, enquanto o restante ser\u00e1 usado para treino.</p> </li> <li> <p>random_state=42: Par\u00e2metro que controla o gerador de n\u00famero aleat\u00f3rios utilizado para sortear os dados antes de separ\u00e1-los. Garante reprodutibilidade.</p> </li> <li> <p>stratify=y: Esse atributo definido como y \u00e9 essencial devido \u00e0 natureza da coluna <code>Wine_Type</code>. Com essa defini\u00e7\u00e3o, ser\u00e1 mantida a mesma propor\u00e7\u00e3o das categorias em ambos os conjuntos, reduzindo o vi\u00e9s.</p> </li> </ul> Sa\u00eddaC\u00f3digo <p>Treino: 142 amostras</p> <p>Teste: 36 amostras</p> <p>Propor\u00e7\u00e3o: 79.8% treino, 20.2% teste</p> <p>Distribui\u00e7\u00e3o das classes - </p> <p>Treino:</p> Wine_Type count Wine Type 1 52 Wine Type 3 49 Wine Type 2 41 <p>Teste:</p> Wine_Type count Wine Type 3 13 Wine Type 1 13 Wine Type 2 10 <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(\"Wine_Type\", axis=1)\nX = scaler.fit_transform(X)\ny = df[\"Wine_Type\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(f\"Treino: {X_train.shape[0]} amostras\\n\")\nprint(f\"Teste: {X_test.shape[0]} amostras\\n\")\nprint(f\"Propor\u00e7\u00e3o: {X_train.shape[0]/X.shape[0]*100:.1f}% treino, {X_test.shape[0]/X.shape[0]*100:.1f}% teste\\n\")\n\nprint(\"Distribui\u00e7\u00e3o das classes - \\n\")\nprint(\"Treino:\\n\")\nprint(y_train.value_counts().to_markdown(), \"\\n\")\nprint(\"Teste:\\n\")\nprint(y_test.value_counts().to_markdown(), \"\\n\")\n</code></pre> <p>Esta divis\u00e3o adequada \u00e9 de extrema import\u00e2ncia, pois ajuda a evitar overfitting.</p>"},{"location":"projeto/main/#etapa-6-treinamento-do-modelo-decision-tree","title":"Etapa 6 - Treinamento do modelo Decision Tree","text":"<p>Agora, vamos treinar um modelo de \u00e1rvore de decis\u00f5es (Decision Tree) para prever a vari\u00e1vel alvo <code>Wine_Type</code> para os dados do conjunto teste. Nosso objetivo aqui \u00e9 treinar e avaliar o modelo, para depois compar\u00e1-lo ao KNN e decidir o melhor para este caso.</p> Decision TreeC\u00f3digo <p></p> <p></p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import tree\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\nX = scaler.fit_transform(X)\ny = df[\"Wine_Type\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nclassifier = tree.DecisionTreeClassifier(random_state=42)\nclassifier.fit(X_train, y_train)\n\nplt.figure(figsize=(12,9))\ntree.plot_tree(\n    classifier,\n    feature_names=pd.DataFrame(X).columns,\n    class_names=classifier.classes_,\n    filled=True,\n    rounded=True,\n    max_depth=3,\n    fontsize=10\n)\nplt.title(\"\u00c1rvore de Decis\u00e3o - Faixas de \u00c1lcool (Vinhos)\")\n\n# plt.savefig(\"docs/projeto/images/d-tree.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre>"},{"location":"projeto/main/#etapa-7-avaliacao-do-modelo-decision-tree","title":"Etapa 7 - Avalia\u00e7\u00e3o do modelo Decision Tree","text":"<p>Agora, vamos realizar a avalia\u00e7\u00e3o do modelo treinado. Primeiramente, vamos ver a acur\u00e1cia do modelo e a import\u00e2ncia de cada uma das features utilizadas para a predi\u00e7\u00e3o.</p> Sa\u00eddaC\u00f3digo <p>Acur\u00e1cia do Modelo: 0.8889 Import\u00e2ncia das Features:  Feature Import\u00e2ncia 12 Proline 0.427374 11 OD280 0.359550 6 Flavanoids 0.113809 9 Color_Intensity 0.076206 1 Malic_Acid 0.018196 0 Alcohol 0.004865 4 Magnesium 0.000000 2 Ash 0.000000 3 Ash_Alcanity 0.000000 8 Proanthocyanins 0.000000 7 Nonflavanoid_Phenols 0.000000 5 Total_Phenols 0.000000 10 Hue 0.000000 </p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import tree\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\nX = scaler.fit_transform(X)\ny = df[\"Wine_Type\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nclassifier = tree.DecisionTreeClassifier(random_state=42)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Acur\u00e1cia do Modelo: {accuracy:.4f}\")\n\nfeature_names = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1).columns\n\nfeature_importance = pd.DataFrame({\n    \"Feature\": feature_names,\n    \"Import\u00e2ncia\": classifier.feature_importances_\n})\nprint(\"&lt;br&gt;Import\u00e2ncia das Features:\")\nprint(feature_importance.sort_values(by=\"Import\u00e2ncia\", ascending=False).to_html() + \"&lt;br&gt;\")\n</code></pre>"},{"location":"projeto/main/#acuracia","title":"Acur\u00e1cia","text":"<p>O modelo atingiu uma boa acur\u00e1cia, de 88,89%, bem pr\u00f3ximo do ideal de 95%. Isso significa que, em 88,89% das previs\u00f5es feitas, o tipo de vinho predito est\u00e1 correto.</p>"},{"location":"projeto/main/#importancia-das-features","title":"Import\u00e2ncia das features","text":"<ul> <li> <p>Na tabela de import\u00e2ncia das features, podemos notar que a vari\u00e1vel mais importante para a previs\u00e3o \u00e9 a <code>Proline</code>, com 42,74% de import\u00e2ncia na previs\u00e3o. </p> </li> <li> <p>Diversas vari\u00e1veis tiveram uma import\u00e2ncia nula, sendo elas: <code>Magnesium</code>, <code>Ash</code>, <code>Ash_Alcanity</code>, <code>Proanthocyanins</code>, <code>Nonflavanoid_Phenols</code>, <code>Total_Phenols</code> e <code>Hue</code></p> </li> <li> <p>As vari\u00e1veis <code>Malic_Acid</code> e <code>Alcohol</code> tiveram uma import\u00e2ncia quase irrelevante na predi\u00e7\u00e3o, de 1,82% e 0,49% respectivamente.</p> </li> </ul>"},{"location":"projeto/main/#matriz-de-confusao","title":"Matriz de Confus\u00e3o","text":"<p>Agora, vamos visualizar a matriz de confus\u00e3o do modelo.</p> Sa\u00eddaC\u00f3digo <p>Matriz de confus\u00e3o</p> <p></p> <p>M\u00e9tricas de qualidade</p> precision recall f1-score support Wine Type 1 0.8 0.92 0.86 13 Wine Type 2 0.9 0.9 0.9 10 Wine Type 3 1 0.85 0.92 13 accuracy 0.89 0.89 0.89 0.89 macro avg 0.9 0.89 0.89 36 weighted avg 0.9 0.89 0.89 36 <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn import tree\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\nX = scaler.fit_transform(X)\ny = df[\"Wine_Type\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nclassifier = tree.DecisionTreeClassifier(random_state=42)\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)\ndisp.plot(cmap=\"Blues\")\nplt.title(\"Matriz de Confus\u00e3o - Wine\")\n# plt.savefig(\"docs/projeto/images/cm-d-tree.svg\", format=\"svg\", transparent=True)\nplt.close()\n\nreport_dict = classification_report(y_test, y_pred, output_dict=True)\nreport_df = pd.DataFrame(report_dict).transpose()\n\nprint(report_df.round(2).to_markdown())\n</code></pre>"},{"location":"projeto/main/#avaliacao-das-metricas","title":"Avalia\u00e7\u00e3o das m\u00e9tricas","text":"<p>Pontos Positivos</p> <ul> <li> <p>Boa acur\u00e1cia geral: 89% - modelo consegue classificar corretamente a maioria das inst\u00e2ncias</p> </li> <li> <p>Excelente precis\u00e3o para Wine Type 3: 100% - quando o modelo classifica como tipo 3, est\u00e1 sempre correto</p> </li> <li> <p>Recall alto para Wine Type 1: 92% - consegue identificar quase todos os vinhos do tipo 1</p> </li> <li> <p>Balanceamento razo\u00e1vel: M\u00e9tricas similares entre as classes</p> </li> </ul> <p>Pontos de Melhoria</p> <p>Problema com Wine Type 3:</p> <ul> <li>Recall de 85% - o modelo falha em identificar 15% dos vinhos do tipo 3</li> </ul> <p>Isso significa que 15% dos vinhos tipo 3 est\u00e3o sendo classificados erroneamente como outros tipos</p> <p>Precis\u00e3o do Wine Type 1:</p> <ul> <li>80% - quando o modelo diz \"\u00e9 tipo 1\", em 20% dos casos est\u00e1 errado</li> </ul>"},{"location":"projeto/main/#etapa-8-treinamento-do-modelo-knn","title":"Etapa 8 - Treinamento do Modelo KNN","text":"<p>Agora, vamos treinar um modelo de KNN para prever a vari\u00e1vel alvo <code>Wine_Type</code> para os dados do conjunto teste. Nosso objetivo aqui \u00e9 treinar e avaliar o modelo, para depois compar\u00e1-lo ao modelo de \u00e1rvore de decis\u00f5es e apontar o modelo superior para este caso.</p> KNNC\u00f3digo <p></p> Acur\u00e1cia: 0.9722  <p></p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\nX = scaler.fit_transform(X)\ny = df[\"Wine_Type\"]\n\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions):.4f}&lt;br&gt;\")\n\n# Visualiza\u00e7\u00e3o do KNN \n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\nprint(\"Variance explained by each component:\", pca.explained_variance_ratio_)\n\nX_train_pca, X_test_pca, _, _ = train_test_split(X_pca, y_encoded, test_size=0.2, random_state=42)\nknn_pca = KNeighborsClassifier(n_neighbors=3)\nknn_pca.fit(X_train_pca, y_train)\n\nh = .05\nx_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\ny_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\nnp.arange(y_min, y_max, h))\n\nZ = knn_pca.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.figure(figsize=(10, 7))\nplt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\nsns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette=\"coolwarm\", edgecolor=\"k\", s=60)\nplt.title(\"KNN com PCA do Modelo 1)\")\nplt.xlabel(\"Componente Principal 1\")\nplt.ylabel(\"Componente Principal 2\")\nplt.legend(title=\"Booking status\")\n\n# plt.savefig(\"docs/projeto/images/knn.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre>"},{"location":"projeto/main/#etapa-9-avaliacao-do-modelo-knn","title":"Etapa 9 - Avalia\u00e7\u00e3o do modelo KNN","text":"<p>Agora, vamos realizar a avalia\u00e7\u00e3o do modelo KNN.</p>"},{"location":"projeto/main/#acuracia_1","title":"Acur\u00e1cia","text":"<p>O modelo alcan\u00e7ou uma acur\u00e1cia de 97,22%, que \u00e9 excelente, contudo indica poss\u00edvel overfitting no modelo. Para testar essa hip\u00f3tese, vamos fazer um teste de acur\u00e1cia nos conjuntos de treino e teste separadamente com KNN e uma valida\u00e7\u00e3o cruzada.</p>"},{"location":"projeto/main/#acuracias-dos-conjuntos-e-validacao-cruzada","title":"Acur\u00e1cias dos conjuntos e valida\u00e7\u00e3o cruzada","text":"Testes overfittingC\u00f3digo <p>Acur\u00e1cias dos conjuntos -</p> <p>Acur\u00e1cia no Treino: 0.9718 </p> <p>Acur\u00e1cia no Teste: 0.9722</p> <p>Valida\u00e7\u00e3o Cruzada (5-fold) -</p> <p>Scores: [0.88888889 0.97222222 0.97222222 1.         0.94285714]</p> <p>M\u00e9dia: 0.9552 (+/- 0.0756)</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.decomposition import PCA\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\nX = scaler.fit_transform(X)\ny = df[\"Wine_Type\"]\n\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nX_train_pca, X_test_pca, _, _ = train_test_split(X_pca, y_encoded, test_size=0.2, random_state=42)\nknn_pca = KNeighborsClassifier(n_neighbors=3)\nknn_pca.fit(X_train_pca, y_train)\n\ntrain_accuracy = knn.score(X_train, y_train)\ntest_accuracy = knn.score(X_test, y_test)\nprint(f\"\\n&lt;b&gt;Acur\u00e1cias dos conjuntos -&lt;/b&gt;\\n\")\nprint(f\"Acur\u00e1cia no Treino: {train_accuracy:.4f} \\n\")\nprint(f\"Acur\u00e1cia no Teste: {test_accuracy:.4f}\")\n\ncv_scores = cross_val_score(knn, X, y_encoded, cv=5)\nprint(f\"\\n&lt;b&gt;Valida\u00e7\u00e3o Cruzada (5-fold) -&lt;/b&gt;\\n\")\nprint(f\"Scores: {cv_scores}\\n\")\nprint(f\"M\u00e9dia: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n</code></pre> <p>Com esses resultados, podemos concluir que h\u00e1 muita chance desse n\u00e3o ser um caso de overfitting. Isso porque as acur\u00e1cias dos conjuntos s\u00e3o consistentes. Al\u00e9m disso, a valida\u00e7\u00e3o cruzada nos demonstrou uma alta m\u00e9dia, de 95,52%, um desvio padr\u00e3o baixo e uma varia\u00e7\u00e3o dos scores entre 88,9% \u00e0 100%, uma varia\u00e7\u00e3o normal.</p>"},{"location":"projeto/main/#matriz-de-confusao_1","title":"Matriz de Confus\u00e3o","text":"Matriz de Confus\u00e3oC\u00f3digo <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\nX = scaler.fit_transform(X)\ny = df[\"Wine_Type\"]\n\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\n\ncm = confusion_matrix(y_test, predictions)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\nplt.xlabel(\"Predito\")\nplt.ylabel(\"Real\")\nplt.title(\"Matriz de Confus\u00e3o - KNN\")\n\n# plt.savefig(\"docs/projeto/images/cm-knn.svg\", format=\"svg\", transparent=True)\nplt.close()\n\nreport_dict = classification_report(y_test, predictions, output_dict=True)\nreport_df = pd.DataFrame(report_dict).transpose()\n\nprint(report_df.round(2).to_markdown())\n</code></pre> <p>O modelo atingiu uma performance excepcional, com acur\u00e1cia geral de 97%, classe 1 perfeitamente prevista pelo modelo com Precis\u00e3o, Recall e F1-Score de 1.00 e alta consist\u00eancia geral, j\u00e1 que todas classes possuem F1-Score acima de 0.96.</p>"},{"location":"projeto/main/#metricas-de-qualidade","title":"M\u00e9tricas de qualidade","text":"precision recall f1-score support 0 1 0.92 0.96 13 1 1 1 1 9 2 0.93 1 0.97 14 accuracy 0.97 0.97 0.97 0.97 macro avg 0.98 0.97 0.98 36 weighted avg 0.97 0.97 0.97 36"},{"location":"projeto/main/#etapa-10-relatorio-final","title":"Etapa 10 - Relat\u00f3rio Final","text":"<p>Ap\u00f3s extensa an\u00e1lise comparativa dos modelos desenvolvidos para a classifica\u00e7\u00e3o de vinhos, o algoritmo K-Nearest Neighbors (KNN) emergiu como a escolha ideal para este problema preditivo, demonstrando performance excepcionalmente superior em todas as m\u00e9tricas de avalia\u00e7\u00e3o. </p> <p>A estrat\u00e9gia de clusteriza\u00e7\u00e3o com K-Means utilizando visualiza\u00e7\u00e3o t-SNE provou-se notavelmente superior \u00e0 abordagem com PCA, oferecendo:</p> <ul> <li> <p>Separa\u00e7\u00e3o mais n\u00edtida entre os clusters de vinhos</p> </li> <li> <p>Preserva\u00e7\u00e3o superior das estruturas locais dos dados</p> </li> <li> <p>Visualiza\u00e7\u00e3o mais intuitiva das rela\u00e7\u00f5es entre as variedades</p> </li> <li> <p>Agrupamentos mais coesos e semanticamente significativos</p> </li> </ul> <p>O t-SNE demonstrou boa capacidade em revelar a estrutura subjacente do dataset, permitindo identificar grupos naturais de vinhos que se alinham perfeitamente com suas caracter\u00edsticas intr\u00ednsecas e qualidade.</p> <p>Embora o K-Means com t-SNE tenha demonstrado resultados promissores, o Silhouette Score de 0.5928 indica espa\u00e7o para otimiza\u00e7\u00e3o, sugerindo que a separa\u00e7\u00e3o entre clusters pode ser aprimorada atrav\u00e9s de outras t\u00e9cnicas como DBSCAN, al\u00e9m do refinamento do pr\u00e9-processamento dos dados e experimenta\u00e7\u00e3o com diferentes redu\u00e7\u00f5es de dimensionalidade.</p>"},{"location":"projeto2/main/","title":"Projeto 2","text":""},{"location":"projeto2/main/#modelo-de-machine-learning-k-means-regressao-linear-multipla-random-forest-e-svm","title":"Modelo de Machine Learning - K-means, Regress\u00e3o Linear M\u00faltipla, Random Forest e SVM","text":"<p>Para esse projeto, foi utilizado um dataset obtido no Kaggle. Os dados usados podem ser baixados aqui, e foram adaptados de outra base de vinhos. Esse projeto utiliza a mesma base do projeto anterior. Portanto, at\u00e9 a Etapa 5, todos os passos s\u00e3o os mesmos.</p>"},{"location":"projeto2/main/#objetivo","title":"Objetivo","text":"<p>O dataset possui informa\u00e7\u00f5es diversas sobre resultados de an\u00e1lises qu\u00edmicas de vinhos produzidos na mesma regi\u00e3o da It\u00e1lia, mas derivados de tr\u00eas diferentes cultivares. A an\u00e1lise determinou as quantidade de 13 constituintes encontrados em cada um dos tr\u00eas tipos de vinho. O objetivo da an\u00e1lise \u00e9 clusterizar a base atrav\u00e9s do k-means e, com os modelos supervisionados, prever o tipo de vinho com base nos dados fornecidos.</p>"},{"location":"projeto2/main/#workflow","title":"Workflow","text":"<p>Os pontos \"etapas\" s\u00e3o o passo-a-passo da realiza\u00e7\u00e3o do projeto.</p>"},{"location":"projeto2/main/#etapa-1-exploracao-de-dados","title":"Etapa 1 - Explora\u00e7\u00e3o de Dados","text":"<p>Primeiramente, para entender melhor a base de dados, vamos descobrir quantas linhas e colunas o dataset possui.</p> Sa\u00eddaC\u00f3digo <p>(178, 13)</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nprint(df.shape)\n</code></pre> <p>Como foi poss\u00edvel observar no c\u00f3digo acima, o dataset possui 178 linhas e 13 colunas, com cada linha possuindo os dados de um vinho.</p>"},{"location":"projeto2/main/#colunas-do-dataset","title":"Colunas do Dataset","text":"<p>Em seguida, \u00e9 necess\u00e1rio descobrir a natureza dos dados. Isso ser\u00e1 feito rodando as linhas de c\u00f3digo abaixo:</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nprint(df.info())\n</code></pre> <p>As informa\u00e7\u00f5es obtidas foram as seguintes:</p> Coluna Tipo Descri\u00e7\u00e3o <code>Alcohol</code> Float Teor alco\u00f3lico do vinho <code>Malic_Acid</code> Float \u00c1cido m\u00e1lico <code>Ash</code> Float Cinzas <code>Ash_Alcanity</code> Float Alcalinidade das cinzas <code>Magnesium</code> Inteiro Magn\u00e9sio <code>Total_Phenols</code> Float Total de fen\u00f3is <code>Flavanoids</code> Float Flavan\u00f3ides <code>Nonflavanoid_Phenols</code> Float Fen\u00f3is n\u00e3o-flavan\u00f3ides <code>Proanthocyanins</code> Float Proantocianidinas <code>Color_Intensity</code> Float Intensidade da cor do vinho <code>Hue</code> Float Satura\u00e7\u00e3o do vinho <code>OD280</code> Float Rela\u00e7\u00e3o OD280/OD315 de vinhos dilu\u00eddos <code>Proline</code> Inteiro Prolina"},{"location":"projeto2/main/#visualizacao-das-variaveis","title":"Visualiza\u00e7\u00e3o das vari\u00e1veis","text":"<p>Em seguida, \u00e9 essencial realizar gr\u00e1ficos para visualizar como cada uma das vari\u00e1veis se comportam, com o objetivo de entender melhor a base da dados. Todas vari\u00e1veis da base s\u00e3o quantitativas, sendo onze cont\u00ednuas e duas discretas.</p>"},{"location":"projeto2/main/#variaveis-quantitativas-continuas","title":"Vari\u00e1veis Quantitativas Cont\u00ednuas","text":"<p>Para cada uma das vari\u00e1veis num\u00e9ricas cont\u00ednuas, ser\u00e1 feito um histograma com o objetivo de visualizar a frequ\u00eancia de valores.</p> AlcoholMalic_AcidAshAsh_AlcanityTotal_PhenolsFlavanoidsNonflavanoid_PhenolsProanthocyaninsColor_IntensityHueOD280 Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:08.152748 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Alcohol\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"red\")\nplt.title(\"Distribui\u00e7\u00e3o do Teor Alco\u00f3lico dos Vinhos - Histograma\")\nplt.xlabel(\"Teor Alco\u00f3lico\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:08.307026 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Malic_Acid\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"yellow\")\nplt.title(\"Distribui\u00e7\u00e3o de \u00c1cido M\u00e1lico dos Vinhos - Histograma\")\nplt.xlabel(\"\u00c1cido M\u00e1lico\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:08.451865 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Ash\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"green\")\nplt.title(\"Distribui\u00e7\u00e3o de Cinzas dos Vinhos - Histograma\")\nplt.xlabel(\"Cinzas\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:08.611863 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Ash_Alcanity\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"blue\")\nplt.title(\"Distribui\u00e7\u00e3o da Alcalinidade das Cinzas dos Vinhos - Histograma\")\nplt.xlabel(\"Alcalinidade das Cinzas\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:08.760206 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Total_Phenols\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"purple\")\nplt.title(\"Distribui\u00e7\u00e3o do Total de Fen\u00f3is dos Vinhos - Histograma\")\nplt.xlabel(\"Total de Fen\u00f3is\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:08.907883 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Flavanoids\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"orange\")\nplt.title(\"Distribui\u00e7\u00e3o de Flavan\u00f3ides dos Vinhos - Histograma\")\nplt.xlabel(\"Flavan\u00f3ides\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:09.052538 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Nonflavanoid_Phenols\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"lightblue\")\nplt.title(\"Distribui\u00e7\u00e3o de Fen\u00f3is n\u00e3o Flavan\u00f3ides dos Vinhos - Histograma\")\nplt.xlabel(\"Fen\u00f3is n\u00e3o Flavan\u00f3ides\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:09.205969 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Proanthocyanins\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"brown\")\nplt.title(\"Distribui\u00e7\u00e3o de Proantocianidinas dos Vinhos - Histograma\")\nplt.xlabel(\"Proantocianidinas\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:09.354215 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Color_Intensity\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"grey\")\nplt.title(\"Distribui\u00e7\u00e3o de Intensidade da Cor dos Vinhos - Histograma\")\nplt.xlabel(\"Intensidade da Cor\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:09.501654 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Hue\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"lightgreen\")\nplt.title(\"Distribui\u00e7\u00e3o de Satura\u00e7\u00e3o dos Vinhos - Histograma\")\nplt.xlabel(\"Satura\u00e7\u00e3o\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:09.654359 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"OD280\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"pink\")\nplt.title(\"Distribui\u00e7\u00e3o do OD280 dos Vinhos - Histograma\")\nplt.xlabel(\"OD280\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre>"},{"location":"projeto2/main/#variaveis-quantitativas-discretas","title":"Vari\u00e1veis Quantitativas Discretas","text":"<p>Para ambas vari\u00e1veis num\u00e9ricas discretas, tamb\u00e9m faremos histogramas.</p> MagnesiumProline Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:09.872373 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Magnesium\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"lightgreen\")\nplt.title(\"Distribui\u00e7\u00e3o de Magn\u00e9sio dos Vinhos - Histograma\")\nplt.xlabel(\"Magn\u00e9sio\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> Gr\u00e1ficoC\u00f3digo 2025-12-05T20:34:10.015432 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(df[\"Proline\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"skyblue\")\nplt.title(\"Distribui\u00e7\u00e3o de Prolina dos Vinhos - Histograma\")\nplt.xlabel(\"Prolina\")\nplt.ylabel(\"Frequ\u00eancia\")\nplt.grid(axis=\"y\", alpha=0.3)\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=False)\nprint(buffer.getvalue())\nplt.close()\n</code></pre> <p>Atrav\u00e9s das an\u00e1lises, foi poss\u00edvel alcan\u00e7ar uma compreens\u00e3o mais aprofundada do funcionamento de cada uma das vari\u00e1veis no dataset, al\u00e9m de haver insights valiosos nesses gr\u00e1ficos.</p>"},{"location":"projeto2/main/#etapa-2-pre-processamento","title":"Etapa 2 - Pr\u00e9-processamento","text":""},{"location":"projeto2/main/#1-passo-identificacao-de-valores-nulos","title":"1\u00b0 Passo: Identifica\u00e7\u00e3o de valores nulos","text":"<p>Atrav\u00e9s da linha de c\u00f3digo abaixo, pode-se identificar que n\u00e3o h\u00e1 valores nulos na base. Portanto, pularemos o passo de tratamento de valores nulos.</p> <pre><code>print(df.isnull().sum())\n</code></pre>"},{"location":"projeto2/main/#2-passo-remocao-de-colunas-desimportantes","title":"2\u00b0 Passo: Remo\u00e7\u00e3o de colunas desimportantes","text":"<p>N\u00e3o h\u00e1 colunas desimportantes para a an\u00e1lise no dataset. Um exemplo de coluna seria um identificador \u00fanico do vinho. Todas s\u00e3o vi\u00e1veis para o modelo de predi\u00e7\u00e3o.</p>"},{"location":"projeto2/main/#3-passo-padronizacao-das-features-numericas","title":"3\u00b0 Passo: Padroniza\u00e7\u00e3o das features num\u00e9ricas","text":"<p>Por fim, \u00e9 necess\u00e1rio padronizar as features num\u00e9ricas da base. Ao inv\u00e9s da normaliza\u00e7\u00e3o, ser\u00e1 utilizada a t\u00e9cnica de padroniza\u00e7\u00e3o devido aos outliers nas features num\u00e9ricas. Para a padroniza\u00e7\u00e3o, foi utilkizado o StandardScaler() do <code>scikit-learn</code>.</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX = scaler.fit_transform(df)\n</code></pre>"},{"location":"projeto2/main/#etapa-3-k-means","title":"Etapa 3 - K-Means","text":"<p>Nessa etapa, realizaremos um modelo K-Means para clusterizar a base e obter categorias que ser\u00e3o a vari\u00e1vel-alvo da previs\u00e3o dos modelos supervisionados. </p>"},{"location":"projeto2/main/#elbow-method","title":"Elbow Method","text":"<p>Antes de treinar o modelo, \u00e9 necess\u00e1rio descobrir o n\u00famero de clusters que ser\u00e1 utilizado. Para isso, aplicaremos o Elbow Method.</p> ElbowC\u00f3digo <p></p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = scaler.fit_transform(df)\n\nwcss = []\nk_range = range(1, 11)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n\nplt.figure(figsize=(10, 6))\nplt.plot(k_range, wcss, \"bo-\", markersize=8, linewidth=2)\nplt.xlabel(\"N\u00famero de Clusters (K)\")\nplt.ylabel(\"WCSS (Within-Cluster Sum of Square)\")\nplt.title(\"Elbow Method - Determinando o K ideal\")\nplt.grid(True, alpha=0.3)\nplt.xticks(k_range)\n\nplt.axvline(x=3, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Poss\u00edvel cotovelo K=3\")\n\nplt.legend()\n# plt.savefig(\"docs/projeto/images/elbow.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre> <p>Podemos observar que o cotovelo est\u00e1 em \\(k = 3\\), logo, esse ser\u00e1 o n\u00famero de clusters utilizado para o K-Means.</p>"},{"location":"projeto2/main/#treinamento-do-k-means","title":"Treinamento do K-Means","text":"<p>Para a forma\u00e7\u00e3o dos clusters do K-Means, foi utilizado a t\u00e9cnica do PCA (Principal Component Analysis). </p> K-Means PCAC\u00f3digo <p></p> Silhouette Score: 0.2849 <p></p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = scaler.fit_transform(df)\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nkmeans = KMeans(n_clusters=3, init=\"k-means++\", random_state=42, n_init=10)\ncluster_labels = kmeans.fit_predict(X)\n\nsilhouette_avg = silhouette_score(X, cluster_labels)\nprint(f\"Silhouette Score: {silhouette_avg:.4f}\")\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap=\"viridis\", alpha=0.7)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker=\"*\", s=300, c=\"red\", label=\"Centroids\")\n\nplt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.2%} var.)\")\nplt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.2%} var.)\")\nplt.title(\"Clusters de Vinhos - K-means (K=3)\")\nplt.legend()\nplt.colorbar(scatter, label=\"Cluster\")\n\n# plt.savefig(\"docs/projeto/images/k-means.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre>"},{"location":"projeto2/main/#etapa-4-avaliacao-do-k-means","title":"Etapa 4 - Avalia\u00e7\u00e3o do K-Means","text":""},{"location":"projeto2/main/#silhouette-score","title":"Silhouette Score","text":"<p>O modelo alcan\u00e7ou um Silhouette Score de 0.2849, indicando uma estrutura de clusters potencialmente artificial. Na escala de -1 a +1, este valor se enquadra na categoria Fraca, por\u00e9m ainda acima do limiar de 0.25 que indicaria aus\u00eancia de clusters naturais.</p>"},{"location":"projeto2/main/#variancia-explicada","title":"Vari\u00e2ncia Explicada","text":"<p>O PCA aplicado para visualiza\u00e7\u00e3o explica 55.41% da vari\u00e2ncia total dos dados. Embora seja uma representa\u00e7\u00e3o simplificada, \u00e9 suficiente para identificar padr\u00f5es gerais, por\u00e9m pode n\u00e3o capturar estruturas mais complexas n\u00e3o-lineares.</p>"},{"location":"projeto2/main/#conclusao-da-avaliacao","title":"Conclus\u00e3o da avalia\u00e7\u00e3o","text":"<p>O silhouette score ficou baixo, por isso, vamos tentar outra t\u00e9cnica que n\u00e3o o PCA. Vamos explorar o t-SNE (t-Distributed Stochastic Neighbor Embedding), uma t\u00e9cnica n\u00e3o-linear que pode revelar melhor estruturas locais e agrupamentos n\u00e3o capturados pelo PCA.</p>"},{"location":"projeto2/main/#re-treinamento-do-k-means","title":"Re-treinamento do k-means","text":"K-Means t-SNEC\u00f3digo Silhouette Score: 0.5928 <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import silhouette_score\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = scaler.fit_transform(df)\n\ntsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\nX_tsne = tsne.fit_transform(X)\n\nkmeans = KMeans(n_clusters=3, init=\"k-means++\", random_state=42, n_init=10)\ncluster_labels = kmeans.fit_predict(X)\n\nsilhouette_avg = silhouette_score(X_tsne, cluster_labels)\nprint(f\"Silhouette Score: {silhouette_avg:.4f}\")\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=cluster_labels, cmap=\"viridis\", alpha=0.7)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n           marker=\"*\", s=300, c=\"red\", label=\"Centroids\")\n\nplt.xlabel(\"t-SNE Component 1\")\nplt.ylabel(\"t-SNE Component 2\")\nplt.title(f\"Clusters de Vinhos - t-SNE + K-means (K=3)\\nSilhouette: {silhouette_avg:.4f}\")\nplt.legend()\nplt.colorbar(scatter, label=\"Cluster\")\n\n# plt.savefig(\"docs/projeto/images/k-means-tsne.svg\", format=\"svg\", transparent=True)\nplt.close()\n</code></pre> <p>Foi poss\u00edvel observar uma melhora significativa no silhouette score, de 0.2849 com PCA para 0.5928 com t-SNE, indicando uma estrutura de clusters muito mais definida.</p> <p>Os centr\u00f3ides na visualiza\u00e7\u00e3o t-SNE podem parecer \"estranhos\" porque esta t\u00e9cnica prioriza a preserva\u00e7\u00e3o de estruturas locais em detrimento de rela\u00e7\u00f5es globais e densidades. O t-SNE distorce intencionalmente o espa\u00e7o para destacar agrupamentos pr\u00f3ximos, o que explica a posi\u00e7\u00e3o n\u00e3o convencional dos centr\u00f3ides na visualiza\u00e7\u00e3o.</p> <p>Em resumo, o trade-off vale a pena: mesmo com a perda de informa\u00e7\u00f5es sobre densidades e estruturas globais, a qualidade da clusteriza\u00e7\u00e3o melhorou drasticamente, revelando padr\u00f5es que n\u00e3o eram aparentes com PCA.</p>"},{"location":"projeto2/main/#criando-a-coluna-da-variavel-alvo","title":"Criando a coluna da vari\u00e1vel-alvo","text":"<p>Agora, vamos criar a coluna que conter\u00e1 a vari\u00e1vel categ\u00f3rica <code>Wine_Type</code>, criada a partir da clusteriza\u00e7\u00e3o do K-Means com t-SNE. Essa coluna ser\u00e1 a vari\u00e1vel-alvo das an\u00e1lises preditivas feitas pelos modelos supervisionados adiante.</p> Sa\u00eddaC\u00f3digo <p>Distribui\u00e7\u00e3o dos clusters - Wine Type 1: 65 Wine Type 2: 51 Wine Type 3: 62</p> <pre><code>import pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-clustering.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = scaler.fit_transform(df)\n\ntsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\nX_tsne = tsne.fit_transform(X)\n\nkmeans = KMeans(n_clusters=3, init=\"k-means++\", random_state=42, n_init=10)\ncluster_labels = kmeans.fit_predict(X)\n\ndf[\"cluster\"] = cluster_labels\ndf[\"Wine_Type\"] = [f\"Wine Type {label + 1}\" for label in cluster_labels]\n\nprint(\"Distribui\u00e7\u00e3o dos clusters -&lt;br&gt;\")\nfor i in range(1, 4):\n    count = len(df[df['Wine_Type'] == f'Wine Type {i}'])\n    print(f\"Wine Type {i}: {count}&lt;br&gt;\")\n\n# Salvar para csv\n# df.to_csv(\"wine-final.csv\", index=False)\n</code></pre> <p>Com isso, podemos partir para as pr\u00f3ximas etapas.</p>"},{"location":"projeto2/main/#etapa-5-divisao-de-dados","title":"Etapa 5 - Divis\u00e3o de dados","text":"<p>Em seguida, vamos realizar a divis\u00e3o dos dados em conjuntos de treino e teste.</p> <ul> <li> <p>Conjunto de Treino: Utilizado para ensinar o modelo a reconhecer padr\u00f5es</p> </li> <li> <p>Conjunto de Teste: Utilizado para avaliar o desempenho do modelo com dados ainda n\u00e3o vistos</p> </li> </ul> <p>Para realizar a divis\u00e3o, foi utilizada a fun\u00e7\u00e3o train_test_split() do <code>scikit-learn</code>. Os par\u00e2metros utilizados s\u00e3o:</p> <ul> <li> <p>test_size=0.2: Define que 20% dos dados ser\u00e3o utilizados para teste, enquanto o restante ser\u00e1 usado para treino.</p> </li> <li> <p>random_state=42: Par\u00e2metro que controla o gerador de n\u00famero aleat\u00f3rios utilizado para sortear os dados antes de separ\u00e1-los. Garante reprodutibilidade.</p> </li> <li> <p>stratify=y: Esse atributo definido como y \u00e9 essencial devido \u00e0 natureza da coluna <code>Wine_Type</code>. Com essa defini\u00e7\u00e3o, ser\u00e1 mantida a mesma propor\u00e7\u00e3o das categorias em ambos os conjuntos, reduzindo o vi\u00e9s.</p> </li> </ul> Sa\u00eddaC\u00f3digo <p>Treino: 142 amostras</p> <p>Teste: 36 amostras</p> <p>Propor\u00e7\u00e3o: 79.8% treino, 20.2% teste</p> <p>Distribui\u00e7\u00e3o das classes - </p> <p>Treino:</p> Wine_Type count Wine Type 1 52 Wine Type 3 49 Wine Type 2 41 <p>Teste:</p> Wine_Type count Wine Type 3 13 Wine Type 1 13 Wine Type 2 10 <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(\"Wine_Type\", axis=1)\nX = scaler.fit_transform(X)\ny = df[\"Wine_Type\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(f\"Treino: {X_train.shape[0]} amostras\\n\")\nprint(f\"Teste: {X_test.shape[0]} amostras\\n\")\nprint(f\"Propor\u00e7\u00e3o: {X_train.shape[0]/X.shape[0]*100:.1f}% treino, {X_test.shape[0]/X.shape[0]*100:.1f}% teste\\n\")\n\nprint(\"Distribui\u00e7\u00e3o das classes - \\n\")\nprint(\"Treino:\\n\")\nprint(y_train.value_counts().to_markdown(), \"\\n\")\nprint(\"Teste:\\n\")\nprint(y_test.value_counts().to_markdown(), \"\\n\")\n</code></pre> <p>Esta divis\u00e3o adequada \u00e9 de extrema import\u00e2ncia, pois ajuda a evitar overfitting.</p>"},{"location":"projeto2/main/#etapa-6-regressao-linear-multipla-do-modelo","title":"Etapa 6 - Regress\u00e3o Linear M\u00faltipla do Modelo","text":"<p>Nessa regress\u00e3o linear m\u00faltipla, vamos prever a vari\u00e1vel quantitativa cont\u00ednua <code>Alcohol</code>, que \u00e9 o teor alco\u00f3lico do vinho.</p> Sa\u00eddaC\u00f3digo <p>Coeficientes da regress\u00e3o:</p> <p>Intercepto = 12.9791</p> <p>Malic_Acid = 0.0772</p> <p>Ash = 0.0081</p> <p>Ash_Alcanity = -0.1001</p> <p>Magnesium = -0.0829</p> <p>Total_Phenols = 0.0798</p> <p>Flavanoids = -0.1193</p> <p>Nonflavanoid_Phenols = -0.0305</p> <p>Proanthocyanins = -0.0325</p> <p>Color_Intensity = 0.3118</p> <p>Hue = 0.1131</p> <p>OD280 = 0.0704</p> <p>Proline = 0.1141</p> <p>Wine_Type = 0.3592</p> <p>R\u00b2 do modelo no conjunto de teste: 0.7847</p> <pre><code>import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\nle = LabelEncoder()\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\ndf[\"Wine_Type\"] = le.fit_transform(df[\"Wine_Type\"])\n\nX = df.drop(columns=[\"Alcohol\", \"cluster\"], axis=1)\ny = df[\"Alcohol\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nrlm = LinearRegression()\nrlm.fit(X_train_scaled, y_train)\n\ncoeficientes = rlm.coef_\nvariaveis = X.columns\n\nprint(\"\\n&lt;b&gt;Coeficientes da regress\u00e3o:&lt;/b&gt;\\n\")\nprint(f\"Intercepto = {round(rlm.intercept_, 4)}\\n\")\nfor i in range(len(coeficientes)):\n    print(f\"{variaveis[i]} = {round(coeficientes[i], 4)}\\n\")\n\ny_pred = rlm.predict(X_test_scaled)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"\\n&lt;b&gt;R\u00b2 do modelo no conjunto de teste: {round(r2, 4)}&lt;/b&gt;\")\n</code></pre> <p>O R\u00b2 do modelo foi de 0,7847, indicando que aproximadamente 78,47% da variabilidade do teor alco\u00f3lico(<code>Alcohol</code>) \u00e9 explicada pelas vari\u00e1veis independentes do modelo. Isso sugere um bom poder explicativo, embora ainda exista uma parcela da variabilidade que n\u00e3o \u00e9 capturada.</p> <ul> <li> <p>Vari\u00e1veis mais relevantes para a predi\u00e7\u00e3o: <code>Wine_Type</code>, <code>Color_Intensity</code> e <code>Flavanoids</code>.</p> </li> <li> <p>Vari\u00e1veis menos relevantes para a predi\u00e7\u00e3o: <code>Ash</code>, <code>Nonflavanoid_Phenols</code> e <code>Proanthocyanins</code>.</p> </li> </ul>"},{"location":"projeto2/main/#etapa-7-treinamento-do-modelo-random-forest","title":"Etapa 7 - Treinamento do Modelo Random Forest","text":"<p>Agora, vamos treinar um modelo de Random Forest para prever a vari\u00e1vel alvo <code>Wine_Type</code> para os dados do conjunto teste. Nosso objetivo aqui \u00e9 treinar e avaliar o modelo, para depois compar\u00e1-lo ao SVM e o KNN (feito no projeto anterior) e decidir o melhor para esta base.</p> Sa\u00eddaC\u00f3digo <p>Accuracy: 0.9722222222222222 Import\u00e2ncia das Features:  Feature Import\u00e2ncia 9 Color_Intensity 0.181092 6 Flavanoids 0.164162 12 Proline 0.144578 11 OD280 0.134655 0 Alcohol 0.105019 10 Hue 0.066951 5 Total_Phenols 0.057426 4 Magnesium 0.036993 3 Ash_Alcanity 0.025609 8 Proanthocyanins 0.025300 1 Malic_Acid 0.024365 2 Ash 0.019025 7 Nonflavanoid_Phenols 0.014824 </p> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nle = LabelEncoder()\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\ny = le.fit_transform(df[\"Wine_Type\"])\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nrf = RandomForestClassifier(n_estimators=100,\n                            max_depth=5,\n                            max_features='sqrt', \n                            random_state=42)\n\nrf.fit(X_train_scaled, y_train)\npredictions = rf.predict(X_test_scaled)\n\nprint(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n\nfeature_importance = pd.DataFrame({\n    \"Feature\": X_train.columns,\n    \"Import\u00e2ncia\": rf.feature_importances_\n})\n\nprint(\"&lt;br&gt;Import\u00e2ncia das Features:\")\nprint(feature_importance.sort_values(by=\"Import\u00e2ncia\", ascending=False).to_html() + \"&lt;br&gt;\")\n</code></pre>"},{"location":"projeto2/main/#etapa-8-avaliacao-do-modelo-random-forest","title":"Etapa 8 - Avalia\u00e7\u00e3o do Modelo Random Forest","text":"<p>Agora, vamos realizar a avalia\u00e7\u00e3o do modelo de Random Forest.</p>"},{"location":"projeto2/main/#acuracia","title":"Acur\u00e1cia","text":"<p>Coincidentemente, o modelo atingiu a mesma acur\u00e1cia do modelo KNN realizado anteriormente, de 97,22%. \u00c9 um \u00f3timo de valor de acur\u00e1cia, por\u00e9m, devemos realizar uma valida\u00e7\u00e3o cruzada novamente para garantir que n\u00e3o \u00e9 overfitting.</p>"},{"location":"projeto2/main/#acuracias-dos-conjuntos-e-validacao-cruzada","title":"Acur\u00e1cias dos conjuntos e valida\u00e7\u00e3o cruzada","text":"Sa\u00eddaC\u00f3digo <p>Acur\u00e1cias dos conjuntos - Random Forest</p> <p>Acur\u00e1cia no Treino: 1.0000 </p> <p>Acur\u00e1cia no Teste: 0.9722</p> <p>Valida\u00e7\u00e3o Cruzada (5-fold) -</p> <p>Scores: [1.         1.         1.         1.         0.94285714]</p> <p>M\u00e9dia: 0.9886 (+/- 0.0457)</p> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nle = LabelEncoder()\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\ny = le.fit_transform(df[\"Wine_Type\"])\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nrf = RandomForestClassifier(n_estimators=100,\n                            max_depth=5,\n                            max_features='sqrt', \n                            random_state=42)\n\nrf.fit(X_train_scaled, y_train)\npredictions = rf.predict(X_test_scaled)\n\ntrain_accuracy = rf.score(X_train_scaled, y_train)\ntest_accuracy = rf.score(X_test_scaled, y_test)\nprint(f\"\\n&lt;b&gt;Acur\u00e1cias dos conjuntos - Random Forest&lt;/b&gt;\\n\")\nprint(f\"Acur\u00e1cia no Treino: {train_accuracy:.4f} \\n\")\nprint(f\"Acur\u00e1cia no Teste: {test_accuracy:.4f}\")\n\ncv_scores = cross_val_score(rf, X, y, cv=5)\nprint(f\"\\n&lt;b&gt;Valida\u00e7\u00e3o Cruzada (5-fold) -&lt;/b&gt;\\n\")\nprint(f\"Scores: {cv_scores}\\n\")\nprint(f\"M\u00e9dia: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n</code></pre> <p>Com esses resultados, assim como aconteceu no KNN, podemos concluir que h\u00e1 muita chance desse n\u00e3o ser um caso de overfitting. Isso porque as acur\u00e1cias dos conjuntos s\u00e3o consistentes. Al\u00e9m disso, a valida\u00e7\u00e3o cruzada nos demonstrou uma alta m\u00e9dia, de 98,86%, um desvio padr\u00e3o baixo e uma varia\u00e7\u00e3o dos scores entre 94,28% \u00e0 100%, uma varia\u00e7\u00e3o normal. Considerando a natureza do dataset, que \u00e9 pequeno, bem separado e pouco ruidoso, o resultado \u00e9 coerente, e nenhum \"milagre\".</p>"},{"location":"projeto2/main/#matriz-de-confusao","title":"Matriz de Confus\u00e3o","text":"Matriz de Confus\u00e3oC\u00f3digo <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.ensemble import RandomForestClassifier\n\nle = LabelEncoder()\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\ny = le.fit_transform(df[\"Wine_Type\"])\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nrf = RandomForestClassifier(n_estimators=100,\n                            max_depth=5,\n                            max_features='sqrt', \n                            random_state=42)\n\nrf.fit(X_train_scaled, y_train)\npredictions = rf.predict(X_test_scaled)\n\ncm = confusion_matrix(y_test, predictions)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\nplt.xlabel(\"Predito\")\nplt.ylabel(\"Real\")\nplt.title(\"Matriz de Confus\u00e3o - Random Forest\")\n\nplt.savefig(\"docs/projeto2/images/cm-rf.svg\", format=\"svg\", transparent=True)\nplt.close()\n\nreport_dict = classification_report(y_test, predictions, output_dict=True)\nreport_df = pd.DataFrame(report_dict).transpose()\n\nprint(report_df.round(2).to_markdown())\n</code></pre> <p>O modelo atingiu uma performance excepcional, com acur\u00e1cia geral de 97%, classe 2 perfeitamente prevista pelo modelo com Precis\u00e3o, Recall e F1-Score de 1.00 e alta consist\u00eancia geral, j\u00e1 que todas classes possuem F1-Score acima de 0.94.</p>"},{"location":"projeto2/main/#metricas-de-qualidade","title":"M\u00e9tricas de qualidade","text":"precision recall f1-score support 0 0.93 1 0.96 13 1 1 0.89 0.94 9 2 1 1 1 14 accuracy 0.97 0.97 0.97 0.97 macro avg 0.98 0.96 0.97 36 weighted avg 0.97 0.97 0.97 36"},{"location":"projeto2/main/#etapa-9-treinamento-do-modelo-svm","title":"Etapa 9 - Treinamento do Modelo SVM","text":"<p>Agora, vamos treinar um modelo SVM para prever a vari\u00e1vel alvo <code>Wine_Type</code> para os dados do conjunto teste. Nosso objetivo aqui \u00e9 treinar e avaliar o modelo, para depois compar\u00e1-lo ao Random Forest e o KNN (feito no projeto anterior) e decidir o melhor para esta base. Para esse primeiro modelo SVM, vamos utilizar o kernel RBF (Radial Basis Function). Esse kernel mapeia os dados em um espa\u00e7o dimensional infinito por meio de uma fun\u00e7\u00e3o gaussiana. Esse kernel foi projetado para grandes volumes de dados, ent\u00e3o, caso haja algum problema, tamb\u00e9m podemos testar um SVM linear.</p> Sa\u00eddaC\u00f3digo <p>Acur\u00e1cia do SVM: 1.0</p> <p>Relat\u00f3rio de Classifica\u00e7\u00e3o:</p> precision recall f1-score support 0 1 1 1 13 1 1 1 1 9 2 1 1 1 14 accuracy 1 1 1 1 macro avg 1 1 1 36 weighted avg 1 1 1 36 <pre><code>import pandas as pd\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\nle = LabelEncoder()\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\ny = le.fit_transform(df[\"Wine_Type\"])\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nsvm_model = SVC(kernel=\"rbf\", C=1, gamma=\"scale\")\n\nsvm_model.fit(X_train_scaled, y_train)\ny_pred = svm_model.predict(X_test_scaled)\n\nacc = accuracy_score(y_test, y_pred)\nprint(\"Acur\u00e1cia do SVM:\", acc)\n\nprint(\"\\nRelat\u00f3rio de Classifica\u00e7\u00e3o:\\n\")\n\nreport_dict = classification_report(y_test, y_pred, output_dict=True)\nreport_df = pd.DataFrame(report_dict).transpose()\n\nprint(report_df.round(2).to_markdown())\n</code></pre>"},{"location":"projeto2/main/#etapa-10-avaliacao-do-modelo-svm","title":"Etapa 10 - Avalia\u00e7\u00e3o do Modelo SVM","text":"<p>Agora, vamos realizar a avalia\u00e7\u00e3o do modelo de SVM com RBF.</p>"},{"location":"projeto2/main/#acuracia_1","title":"Acur\u00e1cia","text":"<p>O modelo atingiu a acur\u00e1cia de 100%. \u00c9 um valor de acur\u00e1cia perfeito, por\u00e9m, devemos realizar uma valida\u00e7\u00e3o cruzada novamente para garantir que n\u00e3o \u00e9 overfitting.</p> Sa\u00eddaC\u00f3digo <p>Acur\u00e1cias dos conjuntos - SVM RBF</p> <p>Acur\u00e1cia no Treino: 1.0000 </p> <p>Acur\u00e1cia no Teste: 1.0000</p> <p>Valida\u00e7\u00e3o Cruzada (5-fold) -</p> <p>Scores: [0.66666667 0.63888889 0.69444444 0.71428571 0.77142857]</p> <p>M\u00e9dia: 0.6971 (+/- 0.0901)</p> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nle = LabelEncoder()\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\ny = le.fit_transform(df[\"Wine_Type\"])\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nsvm_model = SVC(kernel=\"rbf\", C=1, gamma=\"scale\")\n\nsvm_model.fit(X_train_scaled, y_train)\ny_pred = svm_model.predict(X_test_scaled)\n\ntrain_accuracy = svm_model.score(X_train_scaled, y_train)\ntest_accuracy = svm_model.score(X_test_scaled, y_test)\nprint(f\"\\n&lt;b&gt;Acur\u00e1cias dos conjuntos - SVM RBF&lt;/b&gt;\\n\")\nprint(f\"Acur\u00e1cia no Treino: {train_accuracy:.4f} \\n\")\nprint(f\"Acur\u00e1cia no Teste: {test_accuracy:.4f}\")\n\ncv_scores = cross_val_score(svm_model, X, y, cv=5)\nprint(f\"\\n&lt;b&gt;Valida\u00e7\u00e3o Cruzada (5-fold) -&lt;/b&gt;\\n\")\nprint(f\"Scores: {cv_scores}\\n\")\nprint(f\"M\u00e9dia: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n</code></pre> <p>Como \u00e9 poss\u00edvel observar pela acur\u00e1cia dos folds, o modelo sofre de overfitting pesado, j\u00e1 que a m\u00e9dia de acur\u00e1cia ficou de 69,71%. Uma poss\u00edvel causa para esse problema \u00e9 o fato de que, anteriormente, foi criada a coluna <code>Wine_Type</code> atrav\u00e9s de um K-Means realizado em toda a base de dados, potencialmente vazando dados e causando a memoriza\u00e7\u00e3o do treino pelo modelo ao inv\u00e9s da aprendizagem e generaliza\u00e7\u00e3o real. Portanto, agora, vamos realizar direto uma valida\u00e7\u00e3o cruzada mas utilizando o kernel Linear para o SVM.</p> Sa\u00eddaC\u00f3digo <p>Acur\u00e1cias dos conjuntos - SVM linear</p> <p>Acur\u00e1cia no Treino: 1.0000 </p> <p>Acur\u00e1cia no Teste: 1.0000</p> <p>Valida\u00e7\u00e3o Cruzada (5-fold) -</p> <p>Scores: [0.91666667 0.94444444 1.         1.         0.97142857]</p> <p>M\u00e9dia: 0.9665 (+/- 0.0647)</p> <pre><code>import pandas as pd\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nle = LabelEncoder()\nscaler = StandardScaler()\n\ndf = pd.read_csv(\"docs/projeto/wine-final.csv\", sep=\",\", encoding=\"UTF8\")\n\nX = df.drop(columns=[\"Wine_Type\", \"cluster\"], axis=1)\ny = le.fit_transform(df[\"Wine_Type\"])\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nsvm_model = SVC(kernel=\"linear\", C=1, gamma=\"scale\")\n\nsvm_model.fit(X_train_scaled, y_train)\ny_pred = svm_model.predict(X_test_scaled)\n\ntrain_accuracy = svm_model.score(X_train_scaled, y_train)\ntest_accuracy = svm_model.score(X_test_scaled, y_test)\nprint(f\"\\n&lt;b&gt;Acur\u00e1cias dos conjuntos - SVM linear&lt;/b&gt;\\n\")\nprint(f\"Acur\u00e1cia no Treino: {train_accuracy:.4f} \\n\")\nprint(f\"Acur\u00e1cia no Teste: {test_accuracy:.4f}\")\n\ncv_scores = cross_val_score(svm_model, X, y, cv=5)\nprint(f\"\\n&lt;b&gt;Valida\u00e7\u00e3o Cruzada (5-fold) -&lt;/b&gt;\\n\")\nprint(f\"Scores: {cv_scores}\\n\")\nprint(f\"M\u00e9dia: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n</code></pre> <p>O SVM com kernel RBF apresentou overfitting severo em valida\u00e7\u00e3o cruzada (\u224869,7%), enquanto a vers\u00e3o com kernel linear alcan\u00e7ou desempenho robusto (\u224896,6%), evidenciando que, para bases pequenas e aproximadamente lineares, a escolha do kernel \u00e9 determinante para a capacidade de generaliza\u00e7\u00e3o do modelo.</p>"},{"location":"projeto2/main/#matriz-de-confusao_1","title":"Matriz de Confus\u00e3o","text":"<p>Considerando que a acur\u00e1cia do modelo foi de 100%, n\u00e3o \u00e9 sequer necess\u00e1rio obsevar a matriz de confus\u00e3o. O modelo acertou em todos os casos. Todas as m\u00e9tricas foram perfeitas e, dessa vez, atrav\u00e9s do kernel linear, sem overfitting.</p>"},{"location":"projeto2/main/#etapa-11-conclusao-final","title":"Etapa 11 - Conclus\u00e3o Final","text":"<p>Os resultados evidenciam que, em bases pequenas, modelos excessivamente flex\u00edveis tendem a memorizar os dados, enquanto abordagens mais restritivas, como SVM Linear, KNN e Random Forest, apresentam melhor equil\u00edbrio entre vi\u00e9s e vari\u00e2ncia.</p> <p>Em termos de desempenho, considerando a m\u00e9dia da acur\u00e1cia obtida por valida\u00e7\u00e3o cruzada, o SVM Linear apresentou 96,65%, enquanto o Random Forest atingiu a maior m\u00e9dia, com 98,86%. O modelo KNN, por sua vez, apresentou desempenho satisfat\u00f3rio, com m\u00e9dia de 95,52%.</p> <p>Diante desses resultados, conclui-se que o modelo Random Forest \u00e9 o mais adequado para a previs\u00e3o do tipo de vinho nesta base, por apresentar o melhor desempenho m\u00e9dio aliado a maior robustez e estabilidade na generaliza\u00e7\u00e3o.</p>"}]}